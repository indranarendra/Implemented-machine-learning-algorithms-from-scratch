{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PRNN_Assignment1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ZAJ-0muZ62gu"},"outputs":[],"source":["import os\n","import random\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import xml.etree.ElementTree as ET\n","\n","from numpy import ndarray\n","\n","from typing import Type\n","\n","from skimage import io\n","from skimage import transform\n","\n","from matplotlib import pyplot as plt\n","\n","from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, auc"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Ia8sU4Wwanpb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651296749348,"user_tz":-330,"elapsed":22538,"user":{"displayName":"vignan kumar","userId":"03615945832465291219"}},"outputId":"4789a358-16ff-4638-8f6c-c9337d8548ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# PneumoniaMNIST (Binary Class data)"],"metadata":{"id":"F-lATA--Aoes"}},{"cell_type":"code","source":["#Loading PneumoniaMNIST\n","pneu_file_path = \"/content/drive/MyDrive/PRNN/pneumoniamnist.npz\"\n","\n","pneu_data = np.load(pneu_file_path)\n","\n","# Extrating training data\n","pneu_train_data = pneu_data[\"train_images\"]\n","pneu_train_labels = pneu_data[\"train_labels\"]\n","\n","# Extracting test data\n","pneu_test_data = pneu_data[\"test_images\"]\n","pneu_test_labels = pneu_data[\"test_labels\"]\n","\n","# Extracting validation data\n","pneu_val_data = pneu_data[\"val_images\"]\n","pneu_val_labels = pneu_data[\"val_labels\"]\n","\n","# Vectorizing data\n","vector_size = pneu_train_data.shape[1] * pneu_train_data.shape[2]\n","\n","pneu_train_vdata = np.resize(pneu_train_data, (pneu_train_data.shape[0],vector_size))\n","\n","pneu_test_vdata = np.resize(pneu_test_data, (pneu_test_data.shape[0],vector_size))\n","\n","pneu_val_vdata = np.resize(pneu_val_data, (pneu_val_data.shape[0],vector_size))\n","\n","print(pneu_train_vdata.shape)\n","print(pneu_test_vdata.shape)\n","print(pneu_val_vdata.shape)"],"metadata":{"id":"GoClt3zkMIJy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651296754887,"user_tz":-330,"elapsed":7,"user":{"displayName":"vignan kumar","userId":"03615945832465291219"}},"outputId":"280411ee-d775-423c-9b9a-345291794d76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(4708, 784)\n","(624, 784)\n","(524, 784)\n"]}]},{"cell_type":"markdown","source":["#BloodMNIST (MULTI Class Data)"],"metadata":{"id":"gZoBIyobAxe4"}},{"cell_type":"code","source":["# Loading BloodMNIST\n","blood_file_path = \"/content/drive/MyDrive/PRNN/bloodmnist.npz\"\n","\n","blood_data = np.load(blood_file_path)\n","\n","# Extrating training data\n","blood_train_data = blood_data[\"train_images\"]\n","blood_train_labels = blood_data[\"train_labels\"]\n","\n","# Extracting test data\n","blood_test_data = blood_data[\"test_images\"]\n","blood_test_labels = blood_data[\"test_labels\"]\n","\n","# Extracting validation data\n","blood_val_data = blood_data[\"val_images\"]\n","blood_val_labels = blood_data[\"val_labels\"]\n","\n","# Vectorizing data\n","vector_size = blood_train_data.shape[1] * blood_train_data.shape[2]\n","\n","blood_train_vdata = np.resize(blood_train_data, (blood_train_data.shape[0],vector_size))\n","\n","blood_test_vdata = np.resize(blood_test_data, (blood_test_data.shape[0],vector_size))\n","\n","blood_val_vdata = np.resize(blood_val_data, (blood_val_data.shape[0],vector_size))\n","\n","print(blood_train_vdata.shape)\n","print(blood_test_vdata.shape)\n","print(blood_val_vdata.shape)"],"metadata":{"id":"HGv9IMW7Mmqa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651296759674,"user_tz":-330,"elapsed":827,"user":{"displayName":"vignan kumar","userId":"03615945832465291219"}},"outputId":"d78ecd4f-191b-4804-921f-2c232516fb2a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(11959, 784)\n","(3421, 784)\n","(1712, 784)\n"]}]},{"cell_type":"markdown","source":["#Traffic signs Data for bounding box regression problem"],"metadata":{"id":"CcpphcPTBI6B"}},{"cell_type":"code","source":["# Loading Traffic signs data for bounding box regression problem\n","traffic_images_path = \"/content/drive/MyDrive/PRNN/Traffic sign/images\"\n","traffic_annotations_path = \"/content/drive/MyDrive/PRNN/Traffic sign/annotations\"\n","\n","traffic_labels = list()\n","\n","for annot_path in os.listdir(traffic_annotations_path):\n","  traffic_labels.append(annot_path)\n","\n","# Ratio for test data\n","train_prob = 0.7\n","\n","traffic_train_data = list()\n","traffic_test_data = list()\n","traffic_train_gray_data = list()\n","traffic_test_gray_data = list()\n","\n","traffic_train_labels = list()\n","traffic_test_labels = list()\n","\n","for i in range(len(traffic_labels)):\n","  tree = ET.parse(os.path.join(traffic_annotations_path, traffic_labels[i]))\n","  root = tree.getroot()\n","  \n","  img_file = root[1].text\n","\n","  obj = root.find(\"object\")\n","  bndbox = obj.find(\"bndbox\")\n","\n","  if bndbox:\n","    box = np.array([[int(bndbox[0].text), int(bndbox[1].text), int(bndbox[2].text), int(bndbox[3].text)]])\n","\n","    img = io.imread(os.path.join(traffic_images_path, img_file))[:,:,:3]\n","    img_gray = io.imread(os.path.join(traffic_images_path, img_file), as_gray = True)\n","\n","    if img.shape[0] == 400 and img.shape[1] == 300:\n","      img = transform.resize(img, (200,150), anti_aliasing = True)\n","      img_gray = transform.resize(img_gray, (200,150), anti_aliasing = True)\n","\n","      label = box//2\n","\n","      if random.random() <= train_prob:\n","        traffic_train_data.append(img)\n","        traffic_train_gray_data.append(img_gray)\n","        traffic_train_labels.append(label[0])\n","      else:\n","        traffic_test_data.append(img)\n","        traffic_test_gray_data.append(img_gray)\n","        traffic_test_labels.append(label[0])"],"metadata":{"id":"SpZY0LpAP--e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["traffic_train_labels = np.array(traffic_train_labels)\n","traffic_test_labels = np.array(traffic_test_labels)\n","print(traffic_train_labels.shape)\n","print(traffic_test_labels.shape)\n","\n","traffic_train_data = np.array(traffic_train_data)\n","traffic_test_data = np.array(traffic_test_data)\n","traffic_train_gray_data = np.array(traffic_train_gray_data)\n","traffic_test_gray_data = np.array(traffic_test_gray_data)\n","\n","print(traffic_train_data.shape)\n","print(traffic_test_data.shape)\n","print(traffic_test_gray_data.shape)\n","print(traffic_train_gray_data.shape)\n","\n","traffic_train_vdata = np.resize(traffic_train_data, (traffic_train_data.shape[0],400*300*3))\n","traffic_test_vdata = np.resize(traffic_test_data, (traffic_test_data.shape[0],400*300*3))\n","traffic_train_gray_vdata = np.resize(traffic_train_gray_data, (traffic_train_gray_data.shape[0],400*300))\n","traffic_test_gray_vdata = np.resize(traffic_test_gray_data, (traffic_test_gray_data.shape[0],400*300))\n","\n","print()\n","print(traffic_train_vdata.shape)\n","print(traffic_test_vdata.shape)\n","print(traffic_test_gray_vdata.shape)\n","print(traffic_train_gray_vdata.shape)"],"metadata":{"id":"VkUHozPcWjXz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1651293841501,"user_tz":-330,"elapsed":3064,"user":{"displayName":"Pavan Santhosh","userId":"00916621205532500734"}},"outputId":"12d884ee-f3a7-4d19-9380-b5bde420152e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(502, 4)\n","(229, 4)\n","(502, 200, 150, 3)\n","(229, 200, 150, 3)\n","(229, 200, 150)\n","(502, 200, 150)\n","\n","(502, 360000)\n","(229, 360000)\n","(229, 120000)\n","(502, 120000)\n"]}]},{"cell_type":"code","source":["def calc_classification_metrics(y1,y2):\n","  # print(y1.shape,y2.shape)\n","  accuracy = (y1[np.where(y1==y2)].size)/y1.size\n","  print(\"Accuracy:\",accuracy)\n","\n","  try:\n","    f1 = f1_score(y1,y2,average='macro')\n","    print(\"F1 Score:\",f1)\n","  except:\n","    pass\n","\n","  try:\n","    auc = roc_auc_score(y1,y2,multi_class='ovo')\n","    print(\"AUC:\",auc)\n","  except:\n","    pass"],"metadata":{"id":"AlbdiJE9frzg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Density codes\n","GUASSIAN_DENSITY = 0\n","EXPONENTIAL_DENSITY = 1\n","GUASSIAN_MIXTURE_DENSITY = 2\n","\n","class ProbabilityDensity:\n","    def __init__(self) -> None:\n","        pass\n","    \n","    def set_params(self):\n","        pass\n","    \n","    def get_params(self):\n","        pass\n","    \n","    def get_value(self,x : ndarray):\n","        pass\n","\n","class GuassianDensity(ProbabilityDensity):\n","    def __init__(self,dim : int = 1,params : list = None,params_dtype : Type = np.float32) -> None:\n","        self.dim= dim\n","        self.mean = np.ones((dim), dtype = params_dtype)\n","        self.covariance = np.ones((dim,dim), dtype = params_dtype)\n","\n","        if(params != None and self.dim == params[0].shape[0]):\n","            self.set_params(params[0],params[1])\n","\n","        self.const = 1/((2*np.pi)**0.5)**dim\n","\n","        if(dim == 1):\n","            self.det_covariance = self.covariance[0,0]\n","        else:\n","            self.det_covariance = np.linalg.det(self.covariance)\n","        \n","        if(dim == 1 or self.det_covariance == 0):\n","            self.inv_covariance = self.covariance\n","        else:\n","            self.inv_covariance = np.linalg.inv(self.covariance)\n","        \n","    def set_params(self,mean : ndarray, covariance : ndarray):\n","        if(self.mean.shape == mean.shape and self.covariance.shape == covariance.shape):\n","            del self.mean\n","            del self.covariance\n","                \n","            self.mean = mean.copy()\n","            self.covariance = covariance.copy()\n","                \n","            if(self.dim == 1):\n","                self.det_covariance = self.covariance[0,0]\n","            else:\n","                self.det_covariance = np.linalg.det(self.covariance)\n","        \n","            if(self.dim == 1 or self.det_covariance == 0):\n","                self.inv_covariance = self.covariance\n","            else:\n","                self.inv_covariance = np.linalg.inv(self.covariance)\n","        else:\n","            if(self.mean.shape != mean.shape):\n","                print(\"Mismatching dimension for mean: Required\",self.mean.shape,\"Given\",self.mean.shape)\n","            elif(self.covariance.shape != covariance.shape):\n","                print(\"Mismatching dimensions for covariance: Required\",self.covariance.shape,\"Given\",covariance.shape)\n","            \n","    def get_params(self):\n","        return self.mean.copy(), self.covariance.copy()\n","    \n","    def get_value(self,x : ndarray):\n","        if(self.mean.shape == x.shape):\n","            val = self.const\n","            val = val/self.det_covariance\n","            \n","            diff_vec = x - self.mean\n","            \n","            diff_vec = np.expand_dims(diff_vec,axis=0)\n","            \n","            exponent = np.exp(-0.5*(np.matmul(np.matmul(diff_vec,self.inv_covariance),np.transpose(diff_vec))))\n","                \n","            return val*exponent\n","        else:\n","            print(\"Mismatching dimension for input vector: Required\",self.mean.shape,\"Given\",x.shape)\n","            \n","class ExponentialDensity(ProbabilityDensity):\n","    def __init__(self,dim : int = 1,params : ndarray = None,params_dtype : Type = np.float32) -> None:\n","        self.dim = dim\n","        self.lamb = np.ones((dim),dtype=params_dtype)\n","\n","        if(isinstance(params,ndarray) and self.dim == params.shape[0]):\n","            for i in range(self.dim):\n","                self.lamb[i] = params[i]\n","        \n","    def set_params(self,lamb : ndarray):\n","        if(self.lamb.shape == lamb.shape):\n","            del self.lamb\n","            \n","            self.lamb = lamb.copy()\n","        else:\n","            print(\"Mismatching dimension for lambda: Required\",self.lamb.shape,\"Given\",lamb.shape)\n","            \n","    def get_params(self):\n","        return self.lamb.copy()\n","    \n","    def get_value(self,x : ndarray):\n","        if(x.shape == self.lamb.shape):\n","            lamb_prod = 1\n","            \n","            for i in range(self.dim):\n","                lamb_prod = lamb_prod * self.lamb[i]\n","\n","            lamb = self.lamb.copy()\n","            lamb = np.expand_dims(lamb,axis=0)\n","\n","            exponent = np.exp(-1*(np.matmul(self.lamb,np.transpose(x))))\n","\n","            del lamb\n","\n","            return lamb_prod*exponent\n","        else:\n","            print(\"Mismatching dimension for input vector: Required\",self.lamb.shape,\"Given\",x.shape)\n","            \n","class GuassianMixtureDensity(ProbabilityDensity):\n","    def __init__(self, dim : int = 1, num_mixtures : int = 2, params : list = None, params_dtype : Type = np.float32) -> None:\n","        self.dim = dim\n","        self.num_mixtures = num_mixtures\n","        self.latent_params = np.ones((self.num_mixtures))/self.num_mixtures\n","        self.densities = list()\n","        \n","        for i in range(self.num_mixtures):\n","            self.densities.append(GuassianDensity(self.dim))\n","            \n","        if(len(params) == 2):\n","            if(np.sum(params[0]) == 1 and params[0].shape[0] == self.num_mixtures and len(params[1]) == self.num_mixtures):\n","                for i in range(self.num_mixtures):\n","                    self.latent_params[i] = params[0][i]\n","                \n","                i = 0\n","                for param in params[1]:\n","                    if(len(param) == 2):\n","                        self.densities[i].set_params(param[0],param[1])\n","                    else:\n","                        print(\"Warning: Invalid params for\",i,\"th density using default params\")\n","                        \n","                    i += 1\n","        else:\n","            print(\"Error: Invalid number/format of params\")\n","            \n","    def set_params(self,latent_params : ndarray, params : list):\n","        if(np.sum(latent_params) == 1 and latent_params.shape[0] == self.num_mixtures):\n","            del self.latent_params\n","            \n","            self.latent_params = latent_params.copy()\n","        else:\n","            print(\"Warning: Invalid latent params, Ignoring given params\")\n","            \n","        if(len(params) == self.num_mixtures):\n","            i = 0\n","            for param in params:\n","                if(len(params) == 2):\n","                    self.densities[i].set_params(param[0],param[1])\n","                else:\n","                    print(\"Warning: Invalid params for\",i,\"th density, Ignoring given params\")\n","                i += 1\n","        else:\n","            print(\"Error: Mismatch between number of params and mixtures\")\n","            \n","    def add_density(self,latent_params : ndarray, params : list = None):\n","        if(latent_params.shape[0] == self.num_mixtures + 1 and np.sum(latent_params) == 1):\n","            del self.latent_params\n","            \n","            self.latent_params = latent_params.copy()\n","            self.densities.append(GuassianDensity(self.dim))\n","            \n","            if(params != None and len(params) == 2):\n","                self.densities[self.num_mixtures - 1].set_params(params[0],params[1])\n","            \n","    def get_params(self):\n","        params = list()\n","        \n","        for mixture in self.densities:\n","            params.append(mixture.get_params())\n","            \n","        return self.latent_params.copy(), params\n","    \n","    def get_value(self,x : ndarray):\n","        value = 0\n","        \n","        for i in range(self.num_mixtures):\n","            value += self.latent_params[i] * self.densities[i].get_value(x)\n","            \n","        return value"],"metadata":{"id":"efVL_kFUKaWy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Estimate codes\n","ML_ESTIMATE = 0\n","BAYES_ESTIMATE = 1\n","\n","def ML_Estimate(data_points : ndarray, density = GUASSIAN_DENSITY, stretch_dimensions = True):\n","    size, dim = data_points.shape[0], data_points.shape[1:]\n","\n","    if(stretch_dimensions == True):\n","        new_dim = 1\n","        \n","        for i in dim:\n","            new_dim = new_dim * i\n","\n","        new_data = np.zeros((size,new_dim),dtype=data_points.dtype)\n","        \n","        for i in range(size):\n","            new_data[i] = data_points[i].ravel()\n","            \n","        del data_points\n","        \n","        data_points = new_data\n","        \n","        size, dim = data_points.shape\n","\n","    if(density == GUASSIAN_DENSITY):\n","        mean = np.zeros(dim)\n","        \n","        for i in range(size):\n","            mean += data_points[i]\n","        \n","        mean = mean/size\n","        \n","        covariance = np.zeros((dim,dim))\n","        \n","        for i in range(size):\n","            diff = data_points[i] - mean\n","            \n","            diff = np.expand_dims(diff,axis=0)\n","\n","            covariance += np.matmul(np.transpose(diff), diff)\n","            covariance /= size\n","        \n","        return GuassianDensity(dim,[mean, covariance])\n","            \n","    elif(density == EXPONENTIAL_DENSITY):\n","        statistic = np.zeros(dim)\n","        \n","        for i in range(size):\n","            statistic += data_points[i]\n","            \n","        lamb = size/statistic\n","\n","        return ExponentialDensity(dim,lamb)\n","        \n","    elif(density == GUASSIAN_MIXTURE_DENSITY):\n","        pass\n","    else:\n","        print(\"Invalid density model for ML Estimate\")\n","        \n","def MAPEstimate(data_points : ndarray, density : GUASSIAN_DENSITY, stretch_dimensions = True):\n","    pass"],"metadata":{"id":"7Zl4R0EoLAfN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MEAN_SQUARE_LOSS = 0"],"metadata":{"id":"pnWGqMspYhrW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Bayes Classifier"],"metadata":{"id":"Jse1pXKRKVLA"}},{"cell_type":"code","source":["class BayesClassifier:\n","    __IMPLEMENTED_DENSITIES = [GUASSIAN_DENSITY,EXPONENTIAL_DENSITY,GUASSIAN_MIXTURE_DENSITY]\n","    __IMPLEMENTED_ESTIMATES = [ML_ESTIMATE,BAYES_ESTIMATE]\n","    \n","    def __init__(self,num_classes : int, class_densities : list = None, class_priors : ndarray = None) -> None:\n","        self.num_classes = num_classes\n","        \n","        self.__class_densities_codes = list()\n","        \n","        for i in range(self.num_classes):\n","            self.__class_densities_codes.append(GUASSIAN_DENSITY)\n","\n","        if class_densities != None:\n","            for i in range(len(class_densities)):\n","                if class_densities[i] in BayesClassifier.__IMPLEMENTED_DENSITIES:\n","                    self.__class_densities_codes.append(class_densities[i])\n","        \n","        self.class_priors = list()\n","        \n","        if(class_priors != None):\n","            if(self.num_classes == class_priors.shape[0] and np.sum(class_priors) == 1):\n","                for prior in range(self.num_classes):\n","                    self.class_priors.append(prior)\n","            else:\n","                print(\"Invalid class priors: Switching to default\")\n","                \n","        self.train_count = 0\n","                \n","    def train(self, data_points : ndarray, labels : ndarray, estimation_method = ML_ESTIMATE):\n","        if(len(labels.shape) != 1):\n","            print(\"Error: Labels should be 1-d array\")\n","            return\n","        \n","        classes = np.unique(labels)\n","\n","        num_classes = classes.shape[0]\n","\n","        if(self.num_classes != num_classes):\n","            print(\"Number of classes in label set didn't match with classifier classes\")\n","            return\n","        \n","        data_size, dim = data_points.shape[0], data_points[1:]\n","        label_size = labels.shape[0]\n","        \n","        if(data_size == label_size):\n","            if estimation_method not in self.IMPLEMENTED_ESTIMATES:\n","                print(\"Warning: Given estimated method is not implemented in this version yet, defaulting to ML Estimation\")\n","                return\n","                \n","            class_wise_data = dict()\n","            \n","            for i in range(data_size):\n","                if labels[i] in class_wise_data:\n","                    class_wise_data[labels[i]].append(data_points[i])\n","                else:\n","                    class_wise_data[labels[i]] = list()\n","                \n","            if estimation_method == ML_ESTIMATE:\n","                self.class_densities = list()\n","                \n","                for i in range(self.num_classes):\n","                    self.class_densities.append(ML_Estimate(np.array(class_wise_data[i]),self.__class_densities_codes[i]))\n","                    \n","            self.train_count += 1\n","        else:\n","            print(\"Error: Mismatch between number of data points and number of labels provided\")\n","            \n","    def classify(self, data_points : ndarray, true_labes):\n","        data_size, dim = data_points[0], data_points[1:]\n","        \n","        labels = np.zeros((data_size))\n","        probs = np.zeros((self.num_classes))\n","        \n","        for i in range(data_size):\n","            for j in range(self.num_classes):\n","                probs[j] = self.class_densities[j].get_value(data_points[i])\n","                \n","            labels[i] = np.argmax(probs)\n","        \n","\n","        calc_classification_metrics(labels,true_labes)\n","        return labels"],"metadata":{"id":"bo9gt4EvKoVt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifer = BayesClassifier(2)\n","classifier.train(pneu_train_vdata, pneu_train_labels)\n","classifier.classify(pneu_test_vdata, pneu_test_labels)"],"metadata":{"id":"5-Gr1-xb1GVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier = BayesClassifier(2,[EXPONENTIAL_DENSITY, GUASSIAN_DENSITY])\n","classifier.train(pneu_train_vdata, pneu_train_labels)\n","classifier.classify(pneu_test_vdata, pneu_test_labels)"],"metadata":{"id":"wcABSDrR1HQ5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier = BayesClassifier(8)\n","classifier.train(blood_train_vdata, blood_train_labels)\n","classifier.classify(blood_test_vdata, blood_test_labels)"],"metadata":{"id":"Sr2Moyl-1Lu9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier = BayesClassifier(8,[EXPONENTIAL, GUASSIAN_DENSITY, GUASSIAN_DENSITY, GUASSIAN_DESITY, EXPONENTIAL_DENSITY])\n","classifier.train(blood_train_vdata, blood_train_labels)\n","classifier.classify(blood_test_vdata, blood_test_labels)"],"metadata":{"id":"tO6Sx6S31L0M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#K NEAREST NEIGHBOURS"],"metadata":{"id":"fZiu4xeHN3X5"}},{"cell_type":"code","source":["from collections import Counter\n","def euclidean_distance(x1, x2):\n","    return np.sqrt(np.sum((x1 - x2) ** 2))\n","\n","class KNN:\n","    def __init__(self, k=3):\n","        self.k = k\n","\n","    def fit(self, X, y):\n","        self.X_train = X\n","        self.y_train = y\n","\n","    def predict(self, X):\n","        y_pred = [self._predict(x) for x in X]\n","        return np.array(y_pred)\n","\n","    def _predict(self, x):\n","       \n","        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]\n","        \n","        k_idx = np.argsort(distances)[: self.k]\n","        \n","        k_neighbor_labels = [self.y_train[i] for i in k_idx]\n","        \n","        most_common = Counter(k_neighbor_labels).most_common(1)\n","        return most_common[0][0]\n","\n","def normalize(X, axis=-1, order=2):\n","    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n","    l2[l2 == 0] = 1\n","    return X / np.expand_dims(l2, axis)\n","    return X_train, X_test, y_train, y_test\n","\n","def calculate_covariance_matrix(X, Y=None):\n","    if Y is None:\n","        Y = X\n","    n_samples = np.shape(X)[0]\n","    covariance_matrix = (1 / (n_samples-1)) * (X - X.mean(axis=0)).T.dot(Y - Y.mean(axis=0))\n","    return np.array(covariance_matrix, dtype=float)\n","\n","\n","class Plot():\n","    def __init__(self): \n","        self.cmap = plt.get_cmap('viridis')\n","\n","    def _transform(self, X, dim):\n","        covariance = calculate_covariance_matrix(X)\n","        eigenvalues, eigenvectors = np.linalg.eig(covariance) \n","        idx = eigenvalues.argsort()[::-1]\n","        eigenvalues = eigenvalues[idx][:dim]\n","        eigenvectors = np.atleast_1d(eigenvectors[:, idx])[:, :dim]   \n","        X_transformed = X.dot(eigenvectors)\n","        return X_transformed\n","\n","    # Plot the dataset X and the corresponding labels y in 2D using PCA.\n","    def plot_in_2d(self, X, y=None, title=None, accuracy=None, legend_labels=None):\n","        X_transformed = self._transform(X, dim=2)\n","        x1 = X_transformed[:, 0]\n","        x2 = X_transformed[:, 1]\n","        class_distr = []\n","        y = np.array(y).astype(int)\n","        colors = [self.cmap(i) for i in np.linspace(0, 1, len(np.unique(y)))]   \n","        for i, l in enumerate(np.unique(y)):\n","            _x1 = x1[y == l]\n","            _x2 = x2[y == l]\n","            _y = y[y == l]\n","            class_distr.append(plt.scatter(_x1, _x2, color=colors[i]))\n","        if not legend_labels is None: \n","            plt.legend(class_distr, legend_labels, loc=1)\n","        if title:\n","            if accuracy:\n","                perc = 100 * accuracy\n","                plt.suptitle(title)\n","                plt.title(\"Accuracy: %.1f%%\" % perc, fontsize=10)\n","            else:\n","                plt.title(title)\n","        plt.xlabel('Principal Component 1')\n","        plt.ylabel('Principal Component 2')\n","        plt.show()\n","\n","    # Plot the dataset X and the corresponding labels y in 3D using PCA.\n","    # def plot_3d(self,X,y=None):\n","    #     X_transformed = self._transform(X, dim=3)\n","    #     x1 = X_transformed[:, 0]\n","    #     x2 = X_transformed[:, 1]\n","    #     x3 = X_transformed[:, 2]  \n","    #     my_cmap = plt.get_cmap('hsv')\n","    #     fig = plt.figure(figsize = (16, 9))\n","    #     ax = plt.axes(projection =\"3d\")\n","    #     sctt = ax.scatter3D(x1, x2, x3,\n","    #                 alpha = 0.8,\n","    #                 c = (x1 + x2 + x3),\n","    #                 cmap = my_cmap,\n","    #                 marker ='^') \n","    #     plt.title(\"Blood MNIST Dataset\")\n","    #     ax.set_xlabel('Principal Component 1', fontweight ='bold')\n","    #     ax.set_ylabel('Principal Component 2', fontweight ='bold')\n","    #     ax.set_zlabel('Principal Component 3', fontweight ='bold')\n","    #     fig.colorbar(sctt, ax = ax, shrink = 0.5, aspect = 5,) \n","    #     plt.show()\n","def accuracy(y_true, y_pred):\n","    accu = np.sum(y_true == y_pred) / len(y_true)\n","    return accu"],"metadata":{"id":"aeCiwGnOEOV7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["KNN ON PNEUMONIA DATA SET\n"],"metadata":{"id":"1YhppbCeE6Tw"}},{"cell_type":"code","source":["X_train=pneu_train_vdata\n","y_train=np.ravel(pneu_train_labels)\n","X_test=pneu_test_vdata\n","y_test=np.ravel(pneu_test_labels)\n","k = 3\n","clf = KNN(k=k)\n","clf.fit(X_train, y_train)\n","predictions = clf.predict(X_test)\n","calc_classification_metrics(predictions,y_test)\n","print(\"KNN TEST  accuracy\", accuracy(y_test, predictions))\n","Plot().plot_in_2d(X_test, predictions, title=\"K Nearest Neighbors for PNEUMONIA DATA SET \", accuracy=accuracy(y_test,predictions), legend_labels=np.unique(y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":412},"id":"Loza2D1Z77w-","executionInfo":{"status":"ok","timestamp":1651296930521,"user_tz":-330,"elapsed":29570,"user":{"displayName":"vignan kumar","userId":"03615945832465291219"}},"outputId":"cfaf8f16-5c73-4c61-822f-7b7b5a0e0255"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8381410256410257\n","F1 Score: 0.808068411259353\n","AUC: 0.8827253827253827\n","KNN TEST  accuracy 0.8381410256410257\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/matplotlib/collections.py:153: ComplexWarning: Casting complex values to real discards the imaginary part\n","  offsets = np.asanyarray(offsets, float)\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","source":["KNN ON BLOOD MNIST DATASET"],"metadata":{"id":"ZXyYaAf6FbU8"}},{"cell_type":"code","source":["X_train=blood_train_vdata\n","y_train=np.ravel(blood_train_labels)\n","X_test=blood_test_vdata\n","y_test=np.ravel(blood_test_labels)\n","k = 3\n","clf = KNN(k=k)\n","clf.fit(X_train, y_train)\n","predictions = clf.predict(X_test)\n","calc_classification_metrics(predictions,y_test)\n","print(\"KNN classification TEST accuracy\", accuracy(y_test, predictions))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rNTpb6QQC5YH","executionInfo":{"status":"ok","timestamp":1651297321704,"user_tz":-330,"elapsed":389293,"user":{"displayName":"vignan kumar","userId":"03615945832465291219"}},"outputId":"a9ab49d8-0afa-47c7-e64e-6f5258f8b028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.1376790412160187\n","F1 Score: 0.12096017702736828\n","KNN classification TEST accuracy 0.1376790412160187\n"]}]},{"cell_type":"markdown","source":["#Naive Bayes"],"metadata":{"id":"GZC-0uTQLryC"}},{"cell_type":"code","source":["class NaiveBayes:\n","    def fit(self, X, y):\n","        n_samples, n_features = X.shape\n","        self._classes = np.unique(y)\n","        n_classes = len(self._classes)\n","\n","        # calculate mean, var, and prior for each class\n","\n","        self._mean = np.zeros((n_classes, n_features), dtype=np.float64)\n","        self._var = np.zeros((n_classes, n_features), dtype=np.float64)\n","        self._priors = np.zeros(n_classes, dtype=np.float64)\n","\n","        for idx, c in enumerate(self._classes):\n","            X_c = X[y == c]\n","            self._mean[idx, :] = X_c.mean(axis=0)\n","            self._var[idx, :] = X_c.var(axis=0)\n","            self._priors[idx] = X_c.shape[0] / float(n_samples)\n","\n","    def predict(self, X):\n","        y_pred = [self._predict(x) for x in X]\n","        return np.array(y_pred)\n","\n","    def _predict(self, x):\n","        posteriors = []\n","\n","        # calculate posterior probability for each class\n","        for idx, c in enumerate(self._classes):\n","            prior = np.log(self._priors[idx])\n","            posterior = np.sum(np.log(self._pdf(idx, x)))\n","            posterior = prior + posterior\n","            posteriors.append(posterior)\n","\n","        # return class with highest posterior probability\n","        return self._classes[np.argmax(posteriors)]\n","\n","    def _pdf(self, class_idx, x):\n","        mean = self._mean[class_idx]\n","        var = self._var[class_idx]\n","        numerator = np.exp(-((x - mean) ** 2) / (2 * var))\n","        denominator = np.sqrt(2 * np.pi * var)\n","        return numerator / denominator\n","    def accuracy(y_true, y_pred):\n","        accuracy = np.sum(y_true == y_pred) / len(y_true)\n","        return accuracy\n","\n","\n"],"metadata":{"id":"x_viw_7qLuYx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["NAIVE BAYES ON PNEUMONIA DATASET"],"metadata":{"id":"TPcgye1rFhvQ"}},{"cell_type":"code","source":["train_X=pneu_train_vdata\n","train_y=np.ravel(pneu_train_labels)\n","test_X=pneu_test_vdata\n","test_y=np.ravel(pneu_test_labels)\n","nb = NaiveBayes()\n","nb.fit(train_X, train_y)\n","predictions = nb.predict(test_X)\n","#print(\"Naive Bayes classification accuracy\", accuracy(y_test, predictions))\n","calc_classification_metrics(test_y, predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iqPD5XJsFUnv","executionInfo":{"status":"ok","timestamp":1651294456801,"user_tz":-330,"elapsed":411,"user":{"displayName":"Pavan Santhosh","userId":"00916621205532500734"}},"outputId":"94d46920-ce8e-4129-d869-e1fc9ebba34c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.8333333333333334\n","F1 Score: 0.8185701665119712\n","AUC: 0.8128205128205128\n"]}]},{"cell_type":"markdown","source":["NAIVE BAYES ON BLOOD MNIST DATASET"],"metadata":{"id":"K_n0oFVyFzb9"}},{"cell_type":"code","source":["train_X=blood_train_vdata\n","train_y=np.ravel(blood_train_labels)\n","test_X=blood_test_vdata\n","test_y=np.ravel(blood_test_labels)\n","nb = NaiveBayes()\n","nb.fit(train_X, train_y)\n","predictions = nb.predict(test_X)\n","\n","# print(clf.w, clf.b)\n","calc_classification_metrics(test_y, predictions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bIFJUQcbFzAD","executionInfo":{"status":"ok","timestamp":1651294566986,"user_tz":-330,"elapsed":3717,"user":{"displayName":"Pavan Santhosh","userId":"00916621205532500734"}},"outputId":"4cb534a5-1787-4e33-c45b-9827cfe2c6f9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy: 0.11429406606255481\n","F1 Score: 0.07347755105803817\n"]}]},{"cell_type":"markdown","source":["#Linear Models"],"metadata":{"id":"ocxSeqh6YStm"}},{"cell_type":"code","source":["class LinearClassifier:\n","    __IMPLEMENTED_LOSS = [MEAN_SQUARE_LOSS]\n","    \n","    def __init__(self) -> None:\n","        self.kernel = None\n","        self.regularization = [0,0]\n","\n","        self.train_count = 0\n","\n","    def train(self, x_data : ndarray, y_data : ndarray, lr = 0.01, epochs = 1, loss_fn = MEAN_SQUARE_LOSS, kernel = None, regularization = [0,0], shuffle = True):\n","        self.kernel = kernel\n","        self.regularization = regularization\n","\n","        if self.kernel != None:\n","            x_data = self.kernel(x_data)\n","\n","        self.w = np.random.rand(x_data[0].shape[0])\n","        self.b = 0\n","\n","        indices = list(range(x_data.shape[0]))\n","\n","        iters = 0\n","        losses = list()\n","\n","        for epoch in range(epochs):\n","            if shuffle:\n","                random.shuffle(indices)\n","\n","            w_grad = 0\n","            b_grad = 0\n","\n","            loss = list()\n","\n","            l1 = np.sign(self.w)\n","            l2 = 2*self.w\n","            \n","            w_grad = (eval(x_data) - y_data)*x_data\n","            b_grad = (eval(x_data) - y_data)\n","            loss.append((eval(x_data) - y_data)**2)\n","\n","            self.w = self.w - lr*(w_grad + regularization[0]*l1 + regularization[1]*l2)\n","            self.b = self.b - lr*b_grad\n","\n","            losses.append(np.mean(loss))\n","            iters += 1\n","\n","            print(\"Iter: {0}    Loss:{1}\".format(iters,losses[-1]))\n","\n","        plt.plot(list(range(iters)),losses)\n","        plt.xlabel(\"Iterations\")\n","        plt.ylabel(\"Loss\")\n","        plt.show()\n","\n","    def eval(self,x):\n","      y = self.w * x + self.b\n","      print(y.shape)\n","      if y >= 0:\n","        return 1\n","      \n","      return 0\n","\n","class LinearRegression:\n","    __IMPLEMENTED_LOSS = [MEAN_SQUARE_LOSS]\n","\n","    def __init__(self) -> None:\n","        self.kernel = None\n","        self.regularization = [0,0]\n","\n","        self.train_count = 0\n","\n","    def train(self, x_data : ndarray, y_data : ndarray, lr = 0.01, epochs = 1, batch_size = None, loss_fn = MEAN_SQUARE_LOSS, kernel = None, regularization = [0,0], shuffle = True):\n","        self.kernel = kernel\n","        self.regularization = regularization\n","\n","        if self.kernel != None:\n","            x_data = self.kernel(x_data)\n","\n","        \n","\n","    def __activation(self,x):\n","        return x\n","\n","class LogisticRegression(LinearRegression):\n","    def __activation(self,x):\n","        return x"],"metadata":{"id":"6qv7SeCFYUVm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#activation functions\n","class ActivationFunction:\n","    def f(self, x):\n","        raise NotImplementedError\n","\n","    def df(self, x, cached_y=None):\n","        raise NotImplementedError\n","\n","\n","class Identity(ActivationFunction):\n","    def f(self, x):\n","        return x\n","\n","    def df(self, x, cached_y=None):\n","        return np.full(x.shape, 1)\n","\n","\n","class Sigmoid(ActivationFunction):\n","    def f(self, x):\n","        return np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x)))\n","\n","    def df(self, x, cached_y=None):\n","        y = cached_y if cached_y is not None else self.f(x)\n","        return y * (1 - y)\n","\n","\n","class ReLU(ActivationFunction):\n","    def f(self, x):\n","        return np.maximum(0, x)\n","\n","    def df(self, x, cached_y=None):\n","        return np.where(x <= 0, 0, 1)\n","\n","\n","class SoftMax(ActivationFunction):\n","    def f(self, x):\n","        y = np.exp(x - np.max(x, axis=1, keepdims=True))\n","        return y / np.sum(y, axis=1, keepdims=True)\n","\n","    def df(self, x, cached_y=None):\n","        raise NotImplementedError\n","\n","\n","identity = Identity()\n","sigmoid = Sigmoid()\n","relu = ReLU()\n","softmax = SoftMax()"],"metadata":{"id":"ObhWIM0gcEcF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#cost functions\n","epsilon = 1e-20\n","\n","class CostFunction:\n","    def f(self, a_last, y):\n","        raise NotImplementedError\n","\n","    def grad(self, a_last, y):\n","        raise NotImplementedError\n","\n","\n","class SigmoidCrossEntropy(CostFunction):\n","    def f(self, a_last, y):\n","        batch_size = y.shape[0]\n","        a_last = np.clip(a_last, epsilon, 1.0 - epsilon)\n","        cost = -1 / batch_size * (y * np.log(a_last) + (1 - y) * np.log(1 - a_last)).sum()\n","        return cost\n","\n","    def grad(self, a_last, y):\n","        a_last = np.clip(a_last, epsilon, 1.0 - epsilon)\n","        return - (np.divide(y, a_last) - np.divide(1 - y, 1 - a_last))\n","\n","\n","class SoftmaxCrossEntropy(CostFunction):\n","    def f(self, a_last, y):\n","        batch_size = y.shape[0]\n","        cost = -1 / batch_size * (y * np.log(np.clip(a_last, epsilon, 1.0))).sum()\n","        return cost\n","\n","    def grad(self, a_last, y):\n","        return - np.divide(y, np.clip(a_last, epsilon, 1.0))\n","\n","class Squared_error(CostFunction):\n","    def f(self, a_last, y):\n","        batch_size = y.shape[0]\n","        cost = 1 /batch_size * ((y-a_last)**2).sum()\n","        return cost\n","\n","    def grad(self, a_last, y):\n","        return (y-a_last).sum()\n","\n","squared_error = Squared_error()\n","softmax_cross_entropy = SoftmaxCrossEntropy()\n","sigmoid_cross_entropy = SigmoidCrossEntropy()"],"metadata":{"id":"ysZS4j7gcFFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#layers\n","\n","#layer class\n","class Layer:\n","\n","    def init(self, in_dim):\n","        raise NotImplementedError\n","\n","    def forward(self, a_prev, training):\n","        raise NotImplementedError\n","\n","    def backward(self, da):\n","        #Propagates back the gradients\n","        raise NotImplementedError\n","\n","    def update_params(self, dw, db):\n","        raise NotImplementedError\n","    def update_params_L2(self, dw, db):\n","        raise NotImplementedError\n","\n","    def get_params(self):\n","        raise NotImplementedError\n","\n","    def get_output_dim(self):\n","        raise NotImplementedError\n","\n","\n","#convolutional layer\n","\n","class Conv(Layer):\n","\n","    def __init__(self, kernel_size, stride, n_c, padding='valid', activation=identity):\n","        super().__init__()\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.padding = padding\n","        self.pad = None\n","        self.n_h, self.n_w, self.n_c = None, None, n_c\n","        self.n_h_prev, self.n_w_prev, self.n_c_prev = None, None, None\n","        self.w = None\n","        self.b = None\n","        self.activation = activation\n","        self.cache = {}\n","\n","    def init(self, in_dim):\n","        self.pad = 0 if self.padding == 'valid' else int((self.kernel_size - 1) / 2)\n","\n","        self.n_h_prev, self.n_w_prev, self.n_c_prev = in_dim\n","        self.n_h = int((self.n_h_prev - self.kernel_size + 2 * self.pad) / self.stride + 1)\n","        self.n_w = int((self.n_w_prev - self.kernel_size + 2 * self.pad) / self.stride + 1)\n","\n","        self.w = np.random.randn(self.kernel_size, self.kernel_size, self.n_c_prev, self.n_c)\n","        self.b = np.zeros((1, 1, 1, self.n_c))\n","\n","    def forward(self, a_prev, training):\n","        batch_size = a_prev.shape[0]\n","        a_prev_padded = Conv.zero_pad(a_prev, self.pad)\n","        out = np.zeros((batch_size, self.n_h, self.n_w, self.n_c))\n","\n","        # Convolve\n","        for i in range(self.n_h):\n","            v_start = i * self.stride\n","            v_end = v_start + self.kernel_size\n","\n","            for j in range(self.n_w):\n","                h_start = j * self.stride\n","                h_end = h_start + self.kernel_size\n","\n","                out[:, i, j, :] = np.sum(a_prev_padded[:, v_start:v_end, h_start:h_end, :, np.newaxis] *\n","                                         self.w[np.newaxis, :, :, :], axis=(1, 2, 3))\n","\n","        z = out + self.b\n","        a = self.activation.f(z)\n","\n","        if training:\n","            # Cache for backward pass\n","            self.cache.update({'a_prev': a_prev, 'z': z, 'a': a})\n","\n","        return a\n","\n","    def backward(self, da):\n","        batch_size = da.shape[0]\n","        a_prev, z, a = (self.cache[key] for key in ('a_prev', 'z', 'a'))\n","        a_prev_pad = Conv.zero_pad(a_prev, self.pad) if self.pad != 0 else a_prev\n","\n","        da_prev = np.zeros((batch_size, self.n_h_prev, self.n_w_prev, self.n_c_prev))\n","        da_prev_pad = Conv.zero_pad(da_prev, self.pad) if self.pad != 0 else da_prev\n","\n","        dz = da * self.activation.df(z, cached_y=a)\n","        db = 1 / batch_size * dz.sum(axis=(0, 1, 2))\n","        dw = np.zeros((self.kernel_size, self.kernel_size, self.n_c_prev, self.n_c))\n","\n","        # 'Convolve' back\n","        for i in range(self.n_h):\n","            v_start = self.stride * i\n","            v_end = v_start + self.kernel_size\n","\n","            for j in range(self.n_w):\n","                h_start = self.stride * j\n","                h_end = h_start + self.kernel_size\n","\n","                da_prev_pad[:, v_start:v_end, h_start:h_end, :] += \\\n","                    np.sum(self.w[np.newaxis, :, :, :, :] * dz[:, i:i+1, j:j+1, np.newaxis, :], axis=4)\n","\n","                dw += np.sum(a_prev_pad[:, v_start:v_end, h_start:h_end, :, np.newaxis] *\n","                             dz[:, i:i+1, j:j+1, np.newaxis, :], axis=0)\n","\n","        dw /= batch_size\n","\n","        if self.pad != 0:\n","            da_prev = da_prev_pad[:, self.pad:-self.pad, self.pad:-self.pad, :]\n","\n","        return da_prev, dw, db\n","\n","    def get_output_dim(self):\n","        return self.n_h, self.n_w, self.n_c\n","\n","    def update_params(self, dw, db):\n","        self.w -= dw\n","        self.b -= db\n","    def update_params_L2(self, dw, db):\n","        self.w = (-self.w) - dw\n","        self.b -= db\n","\n","    def get_params(self):\n","        return self.w, self.b\n","\n","    @staticmethod\n","    def zero_pad(x, pad):\n","        return np.pad(x, ((0, 0), (pad, pad), (pad, pad), (0, 0)), mode='constant')\n","\n","\n","#dropout\n","class Dropout(Layer):\n","    def __init__(self, keep_prob):\n","        super().__init__()\n","        assert 0 < keep_prob < 1, \"Keep probability must be between 0 and 1\"\n","        self.keep_prob = keep_prob\n","        self.mask_dim = None\n","        self.cached_mask = None\n","\n","    def init(self, in_dim):\n","        self.mask_dim = in_dim\n","\n","    def forward(self, a_prev, training):\n","        if training:\n","            mask = (np.random.rand(*a_prev.shape) < self.keep_prob)\n","            a = self.inverted_dropout(a_prev, mask)\n","\n","            # Cache for backward pass\n","            self.cached_mask = mask\n","\n","            return a\n","\n","        return a_prev\n","\n","    def backward(self, da):\n","        return self.inverted_dropout(da, self.cached_mask), None, None\n","\n","    def update_params(self, dw, db):\n","        pass\n","\n","    def update_params_L2(self, dw, db):\n","        pass\n","\n","    def get_params(self):\n","        pass\n","\n","    def get_output_dim(self):\n","        return self.mask_dim\n","\n","    def inverted_dropout(self, a, mask):\n","        a *= mask\n","        a /= self.keep_prob\n","        return a\n","\n","\n","#fully connected\n","class FullyConnected(Layer):\n","\n","    def __init__(self, size, activation):\n","        super().__init__()\n","        self.size = size\n","        self.activation = activation\n","        self.is_softmax = isinstance(self.activation, SoftMax)\n","        self.cache = {}\n","        self.w = None\n","        self.b = None\n","\n","    def init(self, in_dim):\n","        # weights initialization\n","        self.w = np.random.randn(self.size, in_dim) * np.sqrt(2 / in_dim)\n","\n","        self.b = np.zeros((1, self.size))\n","\n","    def forward(self, a_prev, training):\n","        z = np.dot(a_prev, self.w.T) + self.b\n","        a = self.activation.f(z)\n","\n","        if training:\n","            # Cache for backward pass\n","            self.cache.update({'a_prev': a_prev, 'z': z, 'a': a})\n","\n","        return a\n","\n","    def backward(self, da):\n","        a_prev, z, a = (self.cache[key] for key in ('a_prev', 'z', 'a'))\n","        batch_size = a_prev.shape[0]\n","\n","        if self.is_softmax:\n","            y = da * (-a)\n","\n","            dz = a - y\n","        else:\n","            dz = da * self.activation.df(z, cached_y=a)\n","\n","        dw = 1 / batch_size * np.dot(dz.T, a_prev)\n","        db = 1 / batch_size * dz.sum(axis=0, keepdims=True)\n","        da_prev = np.dot(dz, self.w)\n","\n","        return da_prev, dw, db\n","\n","    def update_params(self, dw, db):\n","        self.w -= dw\n","        self.b -= db\n","\n","    def update_params_L2(self, dw, db):\n","        self.w = (-self.w) - dw\n","        self.b -= db\n","\n","    def get_params(self):\n","        return self.w, self.b\n","\n","    def get_output_dim(self):\n","        return self.size\n","\n","\n","#flatten\n","class Flatten(Layer):\n","\n","    def __init__(self):\n","        super().__init__()\n","        self.original_dim = None\n","        self.output_dim = None\n","\n","    def init(self, in_dim):\n","        self.original_dim = in_dim\n","        self.output_dim = reduce(lambda x, y: x * y, self.original_dim)\n","\n","    def forward(self, a_prev, training):\n","        return a_prev.reshape(a_prev.shape[0], -1)\n","\n","    def backward(self, da):\n","        return da.reshape(da.shape[0], *self.original_dim), None, None\n","\n","    def get_params(self):\n","        pass\n","\n","    def update_params_L2(self, dw, db):\n","        pass\n","\n","    def update_params(self, dw, db):\n","        pass\n","\n","    def get_output_dim(self):\n","        return self.output_dim\n","\n","#pool\n","\n","class Pool(Layer):\n","    \n","    def __init__(self, pool_size, stride, mode):\n","        super().__init__()\n","        self.pool_size = pool_size\n","        self.stride = stride\n","        self.n_h, self.n_w, self.n_c = None, None, None\n","        self.n_h_prev, self.n_w_prev, self.n_c_prev = None, None, None\n","        self.w = None\n","        self.b = None\n","        self.mode = mode\n","        self.cache = {}\n","\n","    def init(self, in_dim):\n","        self.n_h_prev, self.n_w_prev, self.n_c_prev = in_dim\n","        self.n_h = int((self.n_h_prev - self.pool_size) / self.stride + 1)\n","        self.n_w = int((self.n_w_prev - self.pool_size) / self.stride + 1)\n","        self.n_c = self.n_c_prev\n","\n","    def forward(self, a_prev, training):\n","        batch_size = a_prev.shape[0]\n","        a = np.zeros((batch_size, self.n_h, self.n_w, self.n_c))\n","\n","        # Pool\n","        for i in range(self.n_h):\n","            v_start = i * self.stride\n","            v_end = v_start + self.pool_size\n","\n","            for j in range(self.n_w):\n","                h_start = j * self.stride\n","                h_end = h_start + self.pool_size\n","\n","                if self.mode == 'max':\n","                    a_prev_slice = a_prev[:, v_start:v_end, h_start:h_end, :]\n","\n","                    if training:\n","                        # Cache for backward pass\n","                        self.cache_max_mask(a_prev_slice, (i, j))\n","\n","                    a[:, i, j, :] = np.max(a_prev_slice, axis=(1, 2))\n","\n","                elif self.mode == 'average':\n","                    a[:, i, j, :] = np.mean(a_prev[:, v_start:v_end, h_start:h_end, :], axis=(1, 2))\n","\n","                else:\n","                    raise NotImplementedError(\"Invalid type of pooling\")\n","\n","        if training:\n","            self.cache['a_prev'] = a_prev\n","\n","        return a\n","\n","    def backward(self, da):\n","        a_prev = self.cache['a_prev']\n","        batch_size = a_prev.shape[0]\n","        da_prev = np.zeros((batch_size, self.n_h_prev, self.n_w_prev, self.n_c_prev))\n","\n","        # 'Pool' back\n","        for i in range(self.n_h):\n","            v_start = i * self.stride\n","            v_end = v_start + self.pool_size\n","\n","            for j in range(self.n_w):\n","                h_start = j * self.stride\n","                h_end = h_start + self.pool_size\n","\n","                if self.mode == 'max':\n","                    da_prev[:, v_start:v_end, h_start:h_end, :] += da[:, i:i+1, j:j+1, :] * self.cache[(i, j)]\n","\n","                elif self.mode == 'average':\n","                    # Distribute the average value back\n","                    mean_value = np.copy(da[:, i:i+1, j:j+1, :])\n","                    mean_value[:, :, :, np.arange(mean_value.shape[-1])] /= (self.pool_size * self.pool_size)\n","                    da_prev[:, v_start:v_end, h_start:h_end, :] += mean_value\n","\n","                else:\n","                    raise NotImplementedError(\"Invalid type of pooling\")\n","\n","        return da_prev, None, None\n","\n","    def cache_max_mask(self, x, ij):\n","        mask = np.zeros_like(x)\n","        reshaped_x = x.reshape(x.shape[0], x.shape[1] * x.shape[2], x.shape[3])\n","        idx = np.argmax(reshaped_x, axis=1)\n","        ax1, ax2 = np.indices((x.shape[0], x.shape[3]))\n","        mask.reshape(mask.shape[0], mask.shape[1] * mask.shape[2], mask.shape[3])[ax1, idx, ax2] = 1\n","        self.cache[ij] = mask\n","\n","    def update_params(self, dw, db):\n","        pass\n","\n","    def update_params_L2(self, dw, db):\n","        pass\n","\n","    def get_params(self):\n","        pass\n","\n","    def get_output_dim(self):\n","        return self.n_h, self.n_w, self.n_c"],"metadata":{"id":"F1ubAikTcUri"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#optimizer\n","#implemented simple gradient descent approach for optimization\n","class Optimizer:\n","\n","    def __init__(self, trainable_layers):\n","        self.trainable_layers = trainable_layers\n","\n","    def initialize(self):\n","        raise NotImplementedError\n","\n","    def update(self, learning_rate, w_grads, b_grads, step):\n","        raise NotImplementedError\n","\n","\n","class GradientDescent(Optimizer):\n","    def __init__(self, trainable_layers):\n","        Optimizer.__init__(self, trainable_layers)\n","\n","    def initialize(self):\n","        pass\n","\n","    def update(self, learning_rate, w_grads, b_grads, step):\n","        for layer in self.trainable_layers:\n","            layer.update_params(dw=learning_rate * w_grads[layer],\n","                                db=learning_rate * b_grads[layer])\n","            \n","    def update_L2(self, learning_rate, w_grads, b_grads, step):\n","        for layer in self.trainable_layers:\n","            layer.update_params_L2(dw=learning_rate * w_grads[layer],\n","                                db=learning_rate * b_grads[layer])\n","\n","gradient_descent = GradientDescent"],"metadata":{"id":"HdT97fcjcZyH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Neural net\n","# in the train step if we give minibatch size then mini batch based normalization is executed\n","# for dropout normalization I have created a separate class in layers\n","# for L1 and L2 just updated it in the weight udate step\n","from functools import reduce\n","\n","class NeuralNetwork:\n","    def __init__(self, input_dim, layers, cost_function, optimizer=gradient_descent, l2_lambda=0):\n","        self.layers = layers\n","        self.w_grads = {}\n","        self.b_grads = {}\n","        self.cost_function = cost_function\n","        self.optimizer = optimizer\n","        self.l2_lambda = l2_lambda\n","\n","        self.layers[0].init(input_dim)\n","        for prev_layer, curr_layer in zip(self.layers, self.layers[1:]):\n","            curr_layer.init(prev_layer.get_output_dim())\n","\n","        self.trainable_layers = set(layer for layer in self.layers if layer.get_params() is not None)\n","        self.optimizer = optimizer(self.trainable_layers)\n","        self.optimizer.initialize()\n","\n","    def forward_prop(self, x, training=True):\n","        a = x\n","        for layer in self.layers:\n","            a = layer.forward(a, training)\n","\n","        return a\n","\n","    def backward_prop(self, a_last, y):\n","        da = self.cost_function.grad(a_last, y)\n","\n","        batch_size = da.shape[0]\n","\n","        for layer in reversed(self.layers):\n","            da_prev, dw, db = layer.backward(da)\n","\n","            if layer in self.trainable_layers:\n","                if self.l2_lambda != 0:\n","                    self.w_grads[layer] = dw + (self.l2_lambda / batch_size) * layer.get_params()[0]\n","                else:\n","                    self.w_grads[layer] = dw\n","\n","                self.b_grads[layer] = db\n","\n","            da = da_prev\n","\n","    def predict(self, x):\n","        #Calculates the output of the model for the input.\n","        a_last = self.forward_prop(x, training=False)\n","        return a_last\n","\n","    def update_param(self, learning_rate, step):\n","        #Updates the trainable parameters of the layers in the model.\n","        self.optimizer.update(learning_rate, self.w_grads, self.b_grads, step)\n","\n","    def compute_cost(self, a_last, y):\n","        cost = self.cost_function.f(a_last, y)\n","        if self.l2_lambda != 0:\n","            batch_size = y.shape[0]\n","            weights = [layer.get_params()[0] for layer in self.trainable_layers]\n","            l2_cost = (self.l2_lambda / (2 * batch_size)) * reduce(lambda ws, w: ws + np.sum(np.square(w)), weights, 0)\n","            return cost + l2_cost\n","        else:\n","            return cost\n","\n","    def train(self, x_train, y_train, mini_batch_size, learning_rate, num_epochs, validation_data):\n","        x_val, y_val = validation_data\n","        print(f\"Started training [batch_size={mini_batch_size}, learning_rate={learning_rate}]\")\n","        step = 0\n","        for e in range(num_epochs):\n","            print(\"Epoch \" + str(e + 1))\n","            epoch_cost = 0\n","\n","            if mini_batch_size == x_train.shape[0]:\n","                mini_batches = (x_train, y_train)\n","            else:\n","                mini_batches = NeuralNetwork.create_mini_batches(x_train, y_train, mini_batch_size)\n","\n","            num_mini_batches = len(mini_batches)\n","            for i, mini_batch in enumerate(mini_batches, 1):\n","                mini_batch_x, mini_batch_y = mini_batch\n","                step += 1\n","                epoch_cost += self.train_step(mini_batch_x, mini_batch_y, learning_rate, step) / mini_batch_size\n","\n","\n","            print(f\"\\nCost after epoch {e+1}: {epoch_cost}\")\n","\n","            print(\"Computing accuracy on validation set...\")\n","            accuracy = np.sum(np.argmax(self.predict(x_val), axis=1) == np.transpose(y_val)) / x_val.shape[0]\n","            print(f\"Accuracy on validation set: {accuracy}\")\n","\n","            y_pre = self.predict(x_val)\n","            y_pred = abs(np.argmax(y_pre, axis=1))\n","            calc_classification_metrics(y_val[:,0], y_pred)\n","\n","    def train_step(self, x_train, y_train, learning_rate, step):\n","        a_last = self.forward_prop(x_train, training=True)\n","        self.backward_prop(a_last, y_train)\n","        cost = self.compute_cost(a_last, y_train)\n","        self.update_param(learning_rate, step)\n","        return cost\n","\n","    @staticmethod\n","    def create_mini_batches(x, y, mini_batch_size):\n","        batch_size = x.shape[0]\n","        mini_batches = []\n","\n","        p = np.random.permutation(x.shape[0])\n","        x, y = x[p, :], y[p, :]\n","        num_complete_minibatches = batch_size // mini_batch_size\n","\n","        for k in range(0, num_complete_minibatches):\n","            mini_batches.append((\n","                x[k * mini_batch_size:(k + 1) * mini_batch_size, :],\n","                y[k * mini_batch_size:(k + 1) * mini_batch_size, :]\n","            ))\n","\n","        if batch_size % mini_batch_size != 0:\n","            mini_batches.append((\n","                x[num_complete_minibatches * mini_batch_size:, :],\n","                y[num_complete_minibatches * mini_batch_size:, :]\n","            ))\n","\n","        return mini_batches"],"metadata":{"id":"Y328rRXlcdzC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def one_hot(x, num_classes):\n","    out = np.zeros((x.shape[0], num_classes))\n","    out[np.arange(x.shape[0]), x[:, 0]] = 1\n","    return out\n","\n","\n","def preprocess(x_train, y_train, x_test, y_test, channels, classes):\n","    x_train = x_train.reshape(x_train.shape[0], 28, 28, channels).astype(np.float32)\n","    x_test = x_test.reshape(x_test.shape[0], 28, 28, channels).astype(np.float32)\n","    y_train = one_hot(y_train.reshape(y_train.shape[0], 1), classes)\n","    x_train /= 255\n","    x_test /= 255\n","    return x_train, y_train, x_test, y_test"],"metadata":{"id":"-6mxdYIhchaY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, y_train, x_test, y_test = preprocess(pneu_train_data, pneu_train_labels, pneu_test_data, pneu_test_labels, 1, 2)\n","\n","print(\"For Pneumonia dataset\")\n","\n","logistic = NeuralNetwork(\n","    input_dim=(28, 28, 1),\n","    layers=[Flatten(),\n","        FullyConnected(1, identity),\n","        FullyConnected(2, softmax),\n","    ],\n","    cost_function=softmax_cross_entropy,\n","    optimizer=gradient_descent\n",")\n","\n","logistic.train(x_train, y_train,\n","            mini_batch_size=256,\n","            learning_rate=0.001,\n","            num_epochs=4,\n","            validation_data=(x_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEzetwcMc33L","executionInfo":{"status":"ok","timestamp":1651294159137,"user_tz":-330,"elapsed":419,"user":{"displayName":"Pavan Santhosh","userId":"00916621205532500734"}},"outputId":"e49fbdd1-24f5-432f-e9b2-dce617efe077"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["For Pneumonia dataset\n","Started training [batch_size=256, learning_rate=0.001]\n","Epoch 1\n","\n","Cost after epoch 1: 0.04312235335755229\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.625\n","Accuracy: 0.625\n","F1 Score: 0.38461538461538464\n","AUC: 0.5\n","Epoch 2\n","\n","Cost after epoch 2: 0.03927832329784136\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.625\n","Accuracy: 0.625\n","F1 Score: 0.38461538461538464\n","AUC: 0.5\n","Epoch 3\n","\n","Cost after epoch 3: 0.03897970156470444\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.625\n","Accuracy: 0.625\n","F1 Score: 0.38461538461538464\n","AUC: 0.5\n","Epoch 4\n","\n","Cost after epoch 4: 0.0388110906105938\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.625\n","Accuracy: 0.625\n","F1 Score: 0.38461538461538464\n","AUC: 0.5\n"]}]},{"cell_type":"code","source":["x_train, y_train, x_test, y_test = preprocess(blood_train_data, blood_train_labels, blood_test_data, blood_test_labels, 3, 8)\n","\n","print(\"Logistic for BloodMNIST dataset\")\n","\n","logistic = NeuralNetwork(\n","    input_dim=(28, 28, 3),\n","    layers=[Flatten(),\n","        FullyConnected(1, identity),\n","        FullyConnected(8, softmax),\n","    ],\n","    cost_function=softmax_cross_entropy,\n","    optimizer=gradient_descent\n",")\n","\n","logistic.train(x_train, y_train,\n","            mini_batch_size=256,\n","            learning_rate=0.001,\n","            num_epochs=4,\n","            validation_data=(x_test, y_test))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KfUC-dXAfAmc","executionInfo":{"status":"ok","timestamp":1651291914582,"user_tz":-330,"elapsed":2284,"user":{"displayName":"Pavan Santhosh","userId":"00916621205532500734"}},"outputId":"0d037222-0e79-4ed5-bf2d-448ff401f15c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic for BloodMNIST dataset\n","Started training [batch_size=256, learning_rate=0.001]\n","Epoch 1\n","[[-169.50595507   -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -2.07536242   -0.         ...   -0.\n","    -0.           -0.        ]\n"," ...\n"," [-215.08264524   -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [-184.19894105   -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -4.13142539   -0.        ]]\n","[[-0.         -0.         -0.         ... -8.05611953 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -7.87168164 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -8.27245673 -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -7.30914943\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -9.61572235\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.43340671]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.72848139]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.63923922]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -10.29828053]]\n","[[ -0.          -0.         -10.39258778 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -9.11429238]\n"," [ -0.          -0.          -0.         ...  -0.          -7.21797127\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.73707781]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.69689075\n","   -0.        ]\n"," [ -0.          -0.          -9.30525183 ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -4.04036852  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -10.07155283  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -8.76154712\n","   -0.        ]]\n","[[-8.44653269 -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.95052288\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -8.14147783 -0.\n","  -0.        ]\n"," ...\n"," [-0.         -8.28769544 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-7.7611333  -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-10.88777763  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.60590326\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -5.55195901\n","   -0.        ]\n"," [ -0.          -0.         -24.74507882 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -30.01494766 ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -9.3306109   -0.\n","   -0.        ]\n"," [ -0.         -12.46026341  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.15178434]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.86498081]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.         -16.7693519  ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.01211655  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -14.6326859 ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.79149989\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -26.08158659]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.90074158]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -10.48487602\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.         -14.30830701\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.         -19.23394422  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[  -0.           -0.         -271.65127232 ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.          -31.50246345]\n"," ...\n"," [  -0.           -3.31494815   -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -4.34021271   -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -4.61520535   -0.        ]]\n","[[-0.         -0.         -0.         ... -7.90166756 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.50413715\n","  -0.        ]\n"," [-9.1154365  -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.59083686]\n"," [-0.         -0.         -0.         ... -0.         -7.97933305\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.6272106\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.46636342\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -23.79014597]\n"," [ -0.          -0.         -30.21935103 ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -3.70592264 ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -6.53883414  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -12.25264157\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -8.77545402\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.69313796\n","   -0.        ]\n"," [ -0.          -0.         -16.46218618 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.         -46.82858007 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -15.82360109  -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -7.89489314  -0.\n","   -0.        ]\n"," [ -0.         -12.75669952  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.         -13.16320851  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.         -16.90008772  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[  -0.           -0.           -0.         ...   -0.\n","    -0.          -74.77638247]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.          -63.94863356]\n"," [  -0.           -0.         -155.54647809 ...   -0.\n","    -0.           -0.        ]\n"," ...\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.          -53.14563036]]\n","[[ -0.         -10.8475323   -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -5.73414655  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -9.46055882\n","   -0.        ]\n"," ...\n"," [ -0.         -12.91161943  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -8.0106716   -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -4.30897283 ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -32.88450723 ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.87072065\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.34049522\n","   -0.        ]\n"," [-11.44242747  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.12707752]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.8501525 ]\n"," [ -0.          -0.          -0.         ...  -0.          -8.69328874\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -7.93711622  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -8.57882791  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -10.36825947\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.74850876\n","   -0.        ]\n"," [ -0.          -2.8393422   -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.32731882\n","   -0.        ]\n"," ...\n"," [ -0.          -0.         -39.13198595 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -20.96130357]\n"," [ -0.          -3.08714504  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -7.85922311  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -15.11172476\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.85621361]\n"," ...\n"," [ -0.          -0.          -0.         ...  -7.85673892  -0.\n","   -0.        ]\n"," [ -7.00624082  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.47024788\n","   -0.        ]\n"," [ -0.          -2.97959875  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.42722823\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -20.5731091 ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.74099691\n","   -0.        ]\n"," [ -0.          -0.         -29.9718483  ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -6.32335242]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.30588793]\n"," [-9.04050024 -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -7.85819916 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.52679663\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.68790023\n","   -0.        ]\n"," [ -0.          -3.09956291  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.75792615\n","   -0.        ]\n"," [ -0.          -0.         -17.95478768 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -16.32435437  -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.52343692  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -2.15480409 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -8.41526551\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -15.26707075\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ... -13.26422435  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.12089867  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -17.30668096]\n"," [ -0.          -3.15182224  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -5.83824729  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -8.81166404\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.84259096]\n"," ...\n"," [ -0.         -18.71436344  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.05768573]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.86075403\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -15.1948904 ]\n"," [ -0.          -0.         -33.23026296 ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.42145\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -8.39571634 -0.\n","  -0.        ]\n"," [-0.         -7.98027759 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -7.85376128 -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -2.0002446  ... -0.         -0.\n","  -0.        ]\n"," [-0.         -6.59620756 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.37149652\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -19.13462073  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -16.14762795]\n"," ...\n"," [-28.79258743  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.34630373\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -23.33317018]]\n","[[-0.         -6.23177074 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.26669444\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.57986076\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -3.38999222 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.71701317\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.88637954]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -4.7370577  ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -7.26771053 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.08117265\n","   -0.        ]\n"," [-12.69334413  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.05947947]\n"," [ -0.          -0.          -0.         ...  -0.          -9.18173628\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.84369696]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.         -11.3972617\n","   -0.        ]\n"," [ -0.          -6.35714365  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -7.09852559  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -3.51065932  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [-38.49822888  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -16.4457219   -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.         -10.22584217  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -9.228934\n","   -0.        ]\n"," [ -0.          -9.22021532  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -7.49191309  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.83055607]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.65155505]\n"," [ -0.          -0.          -6.25707638 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.98311066  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -5.90036343\n","   -0.        ]\n"," [ -0.          -3.68966678  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -14.11762914  -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.15204527]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -9.3389012  -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -6.11727068 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -9.83050907]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.8793904\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -11.89540461  -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -7.58932175\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.85674547]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -5.15479358 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.47879743\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -4.95269696\n","  -0.        ]\n"," [-0.         -3.69599477 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.3620046\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.10584769 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -9.6157831\n","   -0.        ]\n"," [-10.89808015  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -8.84601756\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -11.29824965\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.864029  ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","  -13.14096595]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -13.72851937]\n"," [ -0.          -0.          -0.         ...  -0.          -4.42130707\n","   -0.        ]\n"," ...\n"," [ -0.          -2.72978724  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.18199031\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -14.4386642 ]]\n","[[ -0.          -0.          -2.27362998 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -2.57046018 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.88982162]\n"," [ -0.          -0.          -0.         ...  -0.         -18.34729309\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.57839994\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -2.80860685  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -15.76054268]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -14.81742921]]\n","[[ -0.          -0.          -0.         ...  -0.          -8.35250711\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.86205558]\n"," [ -0.          -0.          -0.         ...  -0.         -10.88375081\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -9.91343538 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.85665532]]\n","[[ -0.          -0.          -0.         ...  -0.          -5.03225003\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.36875397\n","   -0.        ]\n"," [ -0.          -3.35325565  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -3.67762964  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.66564996  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -13.29115792  -0.\n","   -0.        ]]\n","\n","Cost after epoch 1: 0.3900240166697499\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.2095878398129202\n","Accuracy: 0.2095878398129202\n","F1 Score: 0.2095878398129202\n","Epoch 2\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.70882903\n","  -0.        ]\n"," [-8.27819378 -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.83014483]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.83118018]\n"," [-0.         -0.         -3.64290368 ... -0.         -0.\n","  -0.        ]]\n","[[-25.97270226  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -9.98554059]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -12.49116059  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -11.99210445]]\n","[[ -0.          -0.          -3.92105134 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.83612475]\n"," ...\n"," [ -0.          -0.          -0.         ...  -8.83676283  -0.\n","   -0.        ]\n"," [ -0.          -4.81291215  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -10.90904776\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -4.40503562\n","  -0.        ]\n"," [-0.         -0.         -5.29934839 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.24749799 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -4.84921654\n","  -0.        ]\n"," [-0.         -3.52934089 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.78717733 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -3.40481515 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.43518575\n","   -0.        ]\n"," ...\n"," [ -0.          -5.08069846  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-10.19167792  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.36021158]\n"," [ -0.          -0.         -33.99812392 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -3.39777313  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.79235563  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -8.50207209  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.83515267]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.84005862]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.         -10.5018346\n","   -0.        ]\n"," [-12.77654116  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.         -18.72687526  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.45005142]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.32668289]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -4.99638384  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.31023737\n","   -0.        ]\n"," [-15.29337066  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.98165467]\n"," [ -0.          -6.24333662  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -5.58102001\n","   -0.        ]\n"," [-12.53051835  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -9.39753155  -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ... -10.03496888  -0.\n","   -0.        ]\n"," [ -0.          -0.         -32.65843398 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -10.22961719  -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-12.26069762  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.34546537]]\n","[[-0.         -5.48642209 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.7654576  -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -8.37479642 -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.45199586\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -5.35769889\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -8.39454226]\n"," [-0.         -0.         -0.         ... -0.         -4.67828399\n","  -0.        ]\n"," [-0.         -3.16372123 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -5.87440856]\n"," [-0.         -4.42969682 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.96563355]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -5.24865209 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.33182983\n","  -0.        ]]\n","[[-22.98383058  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -58.30019468 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.0331898   -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -12.60498593  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -7.29920524\n","   -0.        ]]\n","[[ -0.          -0.          -5.38327    ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -7.74957881\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -5.96818438\n","   -0.        ]\n"," [-13.60508594  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -7.94892108  -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -6.26326539\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.31063858\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -9.21508423 -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -8.67348315\n","  -0.        ]\n"," [-0.         -5.37447631 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -9.74066361\n","  -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -4.21518112 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -4.76037259\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.33193967 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -1.6033905  ...  -0.          -0.\n","   -0.        ]\n"," [ -0.         -12.55624028  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -6.97609754  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.06652087]\n"," [ -0.          -7.56400475  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -10.67730233]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.125224  ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -10.2655796 ]]\n","[[ -0.          -6.16031568  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [-11.17470123  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -8.91528195  -0.\n","   -0.        ]\n"," [ -0.          -0.          -1.74433558 ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -3.18802395  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.65485121\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -10.81487715]\n"," ...\n"," [ -0.          -3.09168376  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.58036427  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -16.3899736  ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -8.559442\n","  -0.        ]\n"," [-0.         -0.         -2.31629603 ... -0.         -0.\n","  -0.        ]\n"," [-8.20559356 -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -7.88264477 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.06438793]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.         -13.50090139 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.20319375  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -11.46613093]]\n","[[ -0.          -0.          -0.         ...  -0.         -12.49431909\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -10.83591292\n","   -0.        ]\n"," [ -7.88435185  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.0257229 ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -24.37992507\n","   -0.        ]]\n","[[  -0.           -0.           -0.         ...   -0.\n","    -4.12430044   -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.          -33.61135265]\n"," [  -0.           -2.51424591   -0.         ...   -0.\n","    -0.           -0.        ]\n"," ...\n"," [  -0.           -0.         -284.73672102 ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...  -23.8141243\n","    -0.           -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.83055719]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -6.37330645  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -6.23270851 ...  -0.          -0.\n","   -0.        ]\n"," [-11.07163902  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.23680211]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.99822953]\n"," [ -0.          -3.06279831  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [-25.98785431  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.45925502  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -92.55147687 ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -9.09450416  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.25721541  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [-16.1409607   -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-13.66519155  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -5.08139856  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -3.34644474 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -4.52954814\n","  -0.        ]\n"," [-0.         -3.07201225 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -5.45205121\n","  -0.        ]\n"," [-0.         -2.82552129 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -5.08902781 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -9.35304504\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.69953601]\n"," [-0.         -0.         -0.         ... -0.         -6.3838359\n","  -0.        ]\n"," [-0.         -7.69414179 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ... -10.76919735  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.44767575\n","   -0.        ]\n"," ...\n"," [ -0.          -2.78454007  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -18.17142575  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.17064365]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.3454402   -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -5.46014066  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.38465308\n","   -0.        ]\n"," [ -0.          -0.         -22.53707484 ...  -0.          -0.\n","   -0.        ]]\n","[[-19.23543389  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.00495132  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.55188579\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -17.49264146  -0.\n","   -0.        ]\n"," [ -0.          -3.26460993  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -30.7871002   -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -5.92410023]\n"," [-0.         -5.85858521 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.16346621 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -4.10056616 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.77117311 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-52.27566183  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.24357065  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.373309  ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.49366553\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.673445\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -6.26138014]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.8189407 ]\n"," [-0.         -0.         -0.         ... -0.         -8.90216072\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -9.85965751\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.69370902]\n"," [ -0.          -0.          -0.         ...  -0.          -4.27761097\n","   -0.        ]\n"," ...\n"," [ -0.          -0.         -65.52681709 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.78953585  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -2.03616223 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -7.02947546 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -8.10025788 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.88982982\n","  -0.        ]\n"," [-0.         -0.         -2.36652417 ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.11505112\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.07535028\n","   -0.        ]\n"," [ -0.          -0.         -10.3466879  ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -2.62513788  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-15.30658245  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -14.47732374  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.06925536]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -3.21246272 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.70604658 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.44995167 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.03816653\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -4.41260958\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.15567324]\n"," [ -0.          -0.          -2.04063941 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -10.62318162  -0.\n","   -0.        ]\n"," ...\n"," [-13.60831738  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -8.6035096   -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.1109064 ]\n"," [-0.         -0.         -0.         ... -0.         -7.54792647\n","  -0.        ]\n"," ...\n"," [-0.         -4.73961891 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.90636204]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.36029094\n","   -0.        ]\n"," [-10.04317344  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.17430255\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.02951677\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -7.054704\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -9.48486263 -0.\n","  -0.        ]\n"," [-0.         -0.         -7.13186669 ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -4.59762534  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -2.22065641 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -6.60691725  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -10.02923737  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -9.47441959  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -8.07984921\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ... -14.00011534  -0.\n","   -0.        ]\n"," [ -0.          -3.06935556  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.17594147  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.38563719  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.34601591\n","   -0.        ]]\n","\n","Cost after epoch 2: 0.36683381190526093\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.17684887459807075\n","Accuracy: 0.17684887459807075\n","F1 Score: 0.17684887459807075\n","Epoch 3\n","[[ -0.          -0.          -0.         ...  -0.         -13.79054795\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.65347754]\n"," [ -0.          -0.          -5.51228989 ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -2.40181644 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -9.56486083\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -9.17794082\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.16007794\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -24.18030454  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -16.06347635]\n"," ...\n"," [ -0.          -2.64334886  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.06305458\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.82478748]\n"," [-0.         -0.         -2.72748818 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ... -21.10619328  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.60125582]\n"," [ -0.          -0.          -0.         ... -24.96570376  -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.46975059\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.05566986\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -8.1315867  -0.\n","  -0.        ]\n"," [-0.         -5.58929196 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -6.89209578 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.98776296]\n"," [-0.         -0.         -0.         ... -0.         -8.51104058\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -3.01955337 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.49797083\n","  -0.        ]\n"," ...\n"," [-0.         -3.04658591 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.45379404\n","  -0.        ]\n"," [-0.         -2.70104885 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -2.19071688 ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.74633636 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.98734283]\n"," [ -0.          -0.          -0.         ... -11.12459723  -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.71897779]\n"," [ -0.          -0.          -0.         ...  -0.          -4.74344681\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -5.66691662\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.21204564\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -7.73794433\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.08276906\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.83195135]]\n","[[ -0.          -0.          -0.         ...  -8.79163667  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -8.12942836\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -7.86398634  -0.\n","   -0.        ]\n"," [ -0.          -0.          -3.24663957 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -10.33483572\n","   -0.        ]]\n","[[-0.         -0.         -9.33127071 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.94751841\n","  -0.        ]\n"," [-0.         -0.         -8.34092656 ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.58710972 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[  -0.           -0.         -141.85621442 ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [ -12.88498636   -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," ...\n"," [  -0.           -0.           -0.         ...   -8.03739184\n","    -0.           -0.        ]\n"," [  -0.           -4.72978057   -0.         ...   -0.\n","    -0.           -0.        ]\n"," [ -10.37799383   -0.           -0.         ...   -0.\n","    -0.           -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -8.560965\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -8.37843109 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -8.31019668\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -7.9085497  -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.91805667]\n"," [ -0.          -0.          -0.         ...  -0.          -5.00104131\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.44717852]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -22.97383958 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.20958439]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.31546711\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.80544208]\n"," [-0.         -4.56719677 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -5.98512224 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-39.39438352  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.46031099  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.38595765  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.92731469\n","   -0.        ]\n"," [ -0.          -0.         -94.24336286 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.12030104  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.15348831 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.80865379]\n"," ...\n"," [-0.         -0.         -0.         ... -9.90102476 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -4.00764052 ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -5.64389779\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.83801987  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -3.31256409  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -17.77236341  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -10.56768387  -0.\n","   -0.        ]]\n","[[ -0.          -4.43131752  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.68148384\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.80384304]\n"," [ -0.          -0.          -0.         ...  -0.          -6.9680034\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -16.76150152  -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -6.44511007]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.09450397]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.05589188]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.06937038]\n"," [-0.         -3.61747849 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -5.06645899  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -11.96904951\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.80733252\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -3.84754309 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -6.07019267\n","   -0.        ]\n"," [ -0.          -2.9127532   -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-10.65675749  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.93957124\n","   -0.        ]\n"," [ -0.          -0.          -4.070695   ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.80209771]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -3.86115212 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -5.38287507 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.23857752 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -6.9635748\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.95470146]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.9044857 ]]\n","[[ -0.          -4.11862524  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.83602201  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.58450713\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -25.81842495  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.41088629]]\n","[[ -0.          -0.          -0.         ...  -0.          -9.83901918\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.83244907]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -5.24352505  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-10.6054409   -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -2.78382226  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.94861327  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -3.04526035  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.29306944  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","  -10.18758642]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.57138707]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.50539026]\n"," [ -0.          -0.          -0.         ...  -0.         -10.70767918\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.85140334]\n"," [ -9.41035048  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.         -90.08880944 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.01790986]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -8.25572488 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.72661373  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -8.12489441\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -5.65698108  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -8.51113924  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -10.87347068  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.94374963]]\n","[[-44.3065715   -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.26379136]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.13228934]\n"," [ -0.          -3.55085298  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-22.37491471  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -2.06459994 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.85405676]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.15998254 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.49814907 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -4.03483277 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.80032681]\n"," [-0.         -0.         -0.         ... -0.         -4.8751808\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.76932716\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.79910007]]\n","[[-0.         -4.22676743 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.80580988]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -4.80139546 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.80566427 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -3.55112061 ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.54512785 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -4.09674971 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.42351629\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.82250327]]\n","[[ -0.         -14.61865416  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.79612376]\n"," [ -0.          -3.70852551  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [-16.82240418  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -10.45827669  -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.58140457\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -6.38569935 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-0.         -3.59656697 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.79345787]\n"," [-0.         -0.         -0.         ... -0.         -5.68840719\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.79465685]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.07888932]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.1405994 ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.59599342\n","   -0.        ]\n"," [ -0.          -3.14338468  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.35882027  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-21.1802545   -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -5.65300607 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.41691983\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -8.09352265 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.22138773]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -9.96834156]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -9.61743505]\n"," [ -0.          -2.87532929  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -12.50544098  -0.\n","   -0.        ]\n"," [ -0.          -0.         -11.03229563 ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -16.70089054 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -9.63829614 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.26861627 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -4.77826255\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.33607229]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.85802609]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.79014781]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.80481651]]\n","[[-0. -0. -0. ... -0. -0. -0.]\n"," [-0. -0. -0. ... -0. -0. -0.]\n"," [-0. -0. -0. ... -0. -0. -0.]\n"," ...\n"," [-0. -0. -0. ... -0. -0. -0.]\n"," [-0. -0. -0. ... -0. -0. -0.]\n"," [-0. -0. -0. ... -0. -0. -0.]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.24554685  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.0438307 ]\n"," ...\n"," [-11.09283681  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.52627842\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -12.03067111\n","   -0.        ]]\n","[[-0.         -0.         -3.57806357 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -5.4181506  -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.34481865\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.52462771 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -3.74594198 ... -0.         -0.\n","  -0.        ]]\n","[[-8.92835385 -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -5.27015565 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -5.81269021 -0.         ... -0.         -0.\n","  -0.        ]]\n","\n","Cost after epoch 3: 0.3573496959482048\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.2095878398129202\n","Accuracy: 0.2095878398129202\n","F1 Score: 0.2095878398129202\n","Epoch 4\n","[[-31.2745976   -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -2.68540689  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -3.87373601  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.51534127\n","   -0.        ]\n"," [ -0.          -0.         -12.83210189 ...  -0.          -0.\n","   -0.        ]]\n","[[ -9.29503683  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -16.34236119\n","   -0.        ]\n"," [ -8.3751104   -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.43609232]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.52992311]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -7.34664618]\n"," [-0.         -2.66300968 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -3.11572836 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -4.66542754\n","  -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -5.55824979 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.2416004 ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.19057846\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.24732782]\n"," [-20.45220146  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.6492296\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -8.13564835\n","  -0.        ]\n"," [-0.         -0.         -1.64831377 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -5.2587098  -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.5679856  -0.         ... -0.         -0.\n","  -0.        ]\n"," [-9.52073427 -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[-20.14582833  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -9.27111255  -0.\n","   -0.        ]\n"," [ -0.          -2.87179439  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-13.97545697  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -6.96404563  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -5.02680878  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -5.29626348  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.59401912]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.61821762\n","   -0.        ]\n"," [ -0.          -2.98510756  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.92896427\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -5.13251258\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.49105729\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -17.69330442  -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -8.61801472  -0.\n","   -0.        ]\n"," [ -0.          -4.18464281  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-10.02872797  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -1.76581904 ...  -0.          -0.\n","   -0.        ]\n"," [ -9.78180486  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -4.48458135\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -8.0388452 ]]\n","[[ -0.          -6.30917644  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -9.36932233\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.         -10.34897348\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -13.02285532\n","   -0.        ]\n"," [ -9.84879389  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -4.8680213    -0.        ]\n"," [  -0.           -0.         -717.49821062 ...   -0.\n","    -0.           -0.        ]\n"," ...\n"," [  -0.           -0.           -0.         ...   -0.\n","    -4.41067246   -0.        ]\n"," [  -0.           -0.          -27.77195338 ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.           -5.27699453 ...   -0.\n","    -0.           -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -7.95832482]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.78151968]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.3865778 ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.43605577\n","   -0.        ]\n"," [-41.16483346  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-17.71166729  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.4640634\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.72216144\n","   -0.        ]\n"," ...\n"," [ -0.          -7.47268765  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -6.86044061  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -5.05445859\n","   -0.        ]\n"," [ -0.          -0.         -12.83990577 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -2.60930904  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -2.81656741  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -13.35921243  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -2.58965913 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.82374249 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-9.3126033  -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.29760393\n","  -0.        ]]\n","[[-0.         -3.51655816 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.16838487 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -4.05577268 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.94627998]\n"," [-0.         -3.65921614 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.0550972   -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.82323607]\n"," ...\n"," [ -0.          -0.          -0.         ... -12.66620967  -0.\n","   -0.        ]\n"," [ -0.          -3.73879159  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -4.87217598  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-32.9222729   -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.01209183  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -18.52814198  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -7.04806265\n","   -0.        ]\n"," [-25.54257893  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -9.56044793  -0.\n","   -0.        ]\n"," [ -0.          -3.75554798  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -3.3390862  ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [-16.48146633  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -3.65767478  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -7.136938\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -5.52800604\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.97717523]\n"," [-0.         -0.         -0.         ... -0.         -4.85042712\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -5.41182743\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.00119316\n","  -0.        ]\n"," [-0.         -3.20565745 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.84115458]\n"," [ -0.          -8.79472343  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -6.97287529  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -10.41331828  -0.\n","   -0.        ]\n"," [ -0.          -0.          -2.49341203 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -2.41547922 ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -3.213416   -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.47956453 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.12853734 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -3.94762265 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.16223801]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -6.89780818\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -7.9742349   -0.\n","   -0.        ]\n"," [ -9.52119403  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.         -12.37581081\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.70189436\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -9.77534054  -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.8125749 ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.55855246\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-26.70404329  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.8183512\n","   -0.        ]]\n","[[-10.74601251  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.57047118]\n"," [ -0.          -0.          -0.         ...  -0.          -5.43363215\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -5.85937368]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.35850954]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -12.74835032  -0.\n","   -0.        ]]\n","[[-0.         -0.         -3.6072684  ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.78155635]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.92720577]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.9935308  -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.78898655]]\n","[[ -0.          -0.          -0.         ...  -0.          -6.62143751\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -11.60519918  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.         -10.08389876\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -11.5104175   -0.\n","   -0.        ]]\n","[[-0.         -0.         -2.30001079 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.79084809]\n"," [-0.         -4.50553439 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -5.65226558\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -7.90669038\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ... -13.6672694   -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.58510821\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -5.27981072 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.49949351\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -1.75911044 ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.98784398 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -6.18626145 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.11851544\n","  -0.        ]\n"," [-0.         -4.9867601  -0.         ... -0.         -0.\n","  -0.        ]]\n","[[  -0.           -0.           -0.         ...   -0.\n","    -0.           -7.31079734]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -4.1125428    -0.        ]\n"," [  -0.           -0.           -0.         ...   -0.\n","    -0.           -0.        ]\n"," ...\n"," [  -0.           -2.59511761   -0.         ...   -0.\n","    -0.           -0.        ]\n"," [  -0.           -0.         -733.45674245 ...   -0.\n","    -0.           -0.        ]\n"," [ -65.88750149   -0.           -0.         ...   -0.\n","    -0.           -0.        ]]\n","[[-0.         -0.         -0.         ... -0.         -0.\n","  -6.4568554 ]\n"," [-0.         -4.32249816 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -7.53879697\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -9.80722664\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -8.04012611]\n"," [ -0.          -0.          -0.         ...  -0.          -4.31982778\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -7.08046783]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -4.29345227\n","   -0.        ]\n"," [ -0.          -2.58412987  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [-69.94373391  -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.32522741  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -4.63493094  -0.         ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [-11.10549966  -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.58142035\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.         -12.23239118 ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -4.45714857\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -4.36000663 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -6.551584  ]\n"," [-0.         -5.06603493 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -5.00608156\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -8.09311652 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.90976121]]\n","[[ -0.          -0.          -0.         ...  -0.          -4.8357716\n","   -0.        ]\n"," [ -0.          -2.70581116  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.20959868]\n"," ...\n"," [ -0.          -4.09486302  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -12.27237872  -0.\n","   -0.        ]\n"," [ -0.          -2.75953395  -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -0.         -0.         ... -8.83927238 -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.9626449 ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -6.5200642\n","  -0.        ]\n"," [-0.         -4.32960897 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -2.80462047  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -4.28006561 ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -14.42874788  -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -2.82204883  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -4.212567   ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]]\n","[[-0.         -7.84444038 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -8.64370106 -0.\n","  -0.        ]\n"," [-0.         -3.68212509 -0.         ... -0.         -0.\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -3.70924715 -0.         ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -3.02434564  -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -0.\n","   -6.05193958]\n"," [ -0.          -0.          -0.         ...  -0.          -5.14594873\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ... -23.14279269  -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.47466028\n","   -0.        ]\n"," [ -0.          -0.          -0.         ...  -0.          -5.00569856\n","   -0.        ]]\n","[[-0.         -3.85959121 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -4.87503735 -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -0.         ... -0.         -6.4353899\n","  -0.        ]\n"," ...\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -5.80481574]\n"," [-0.         -0.         -0.         ... -0.         -0.\n","  -0.        ]\n"," [-0.         -0.         -1.99133111 ... -0.         -0.\n","  -0.        ]]\n","[[ -0.          -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -17.55166313  -0.\n","   -0.        ]\n"," [ -0.          -0.          -3.31616746 ...  -0.          -0.\n","   -0.        ]\n"," ...\n"," [ -0.          -0.          -0.         ...  -0.          -5.29542552\n","   -0.        ]\n"," [-49.1939164   -0.          -0.         ...  -0.          -0.\n","   -0.        ]\n"," [ -0.          -0.          -0.         ... -21.72230613  -0.\n","   -0.        ]]\n","\n","Cost after epoch 4: 0.3602526801483163\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.24028061970184156\n","Accuracy: 0.24028061970184156\n","F1 Score: 0.24028061970184156\n","Finished training\n"]}]},{"cell_type":"code","source":["def preprocess1(x_train, y_train, x_test, y_test, channels, classes):\n","    x_train = x_train.reshape(x_train.shape[0], 200, 150, channels).astype(np.float32)\n","    x_test = x_test.reshape(x_test.shape[0], 200, 150, channels).astype(np.float32)\n","    return x_train, y_train, x_test, y_test"],"metadata":{"id":"2qkLW-VMlFwx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x_train, y_train, x_test, y_test = preprocess1(traffic_train_data, traffic_train_labels, traffic_test_data, traffic_test_labels, 3, 4)\n","\n","print(\"Bounding box Regression\")\n","\n","logistic = NeuralNetwork(\n","    input_dim=(200, 150, 3),\n","    layers=[Flatten(),\n","        FullyConnected(1, identity),\n","        FullyConnected(4, relu),\n","    ],\n","    cost_function=softmax_cross_entropy,\n","    optimizer=gradient_descent\n",")\n","\n","logistic.train(x_train, y_train,\n","            mini_batch_size=1,\n","            learning_rate=0.001,\n","            num_epochs=10000,\n","            validation_data=(x_test, y_test))"],"metadata":{"id":"sUVJYITHfesW","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1651294291333,"user_tz":-330,"elapsed":94835,"user":{"displayName":"Pavan Santhosh","userId":"00916621205532500734"}},"outputId":"3d8545b4-64d2-40d8-a226-10556b06fd7d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bounding box Regression\n","Started training [batch_size=1, learning_rate=0.001]\n","Epoch 1\n","\n","Cost after epoch 1: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 2\n","\n","Cost after epoch 2: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 3\n","\n","Cost after epoch 3: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 4\n","\n","Cost after epoch 4: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 5\n","\n","Cost after epoch 5: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 6\n","\n","Cost after epoch 6: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 7\n","\n","Cost after epoch 7: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 8\n","\n","Cost after epoch 8: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 9\n","\n","Cost after epoch 9: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 10\n","\n","Cost after epoch 10: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 11\n","\n","Cost after epoch 11: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 12\n","\n","Cost after epoch 12: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 13\n","\n","Cost after epoch 13: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 14\n","\n","Cost after epoch 14: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 15\n","\n","Cost after epoch 15: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 16\n","\n","Cost after epoch 16: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 17\n","\n","Cost after epoch 17: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 18\n","\n","Cost after epoch 18: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 19\n","\n","Cost after epoch 19: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 20\n","\n","Cost after epoch 20: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 21\n","\n","Cost after epoch 21: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 22\n","\n","Cost after epoch 22: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 23\n","\n","Cost after epoch 23: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 24\n","\n","Cost after epoch 24: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 25\n","\n","Cost after epoch 25: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 26\n","\n","Cost after epoch 26: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 27\n","\n","Cost after epoch 27: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 28\n","\n","Cost after epoch 28: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 29\n","\n","Cost after epoch 29: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 30\n","\n","Cost after epoch 30: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 31\n","\n","Cost after epoch 31: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 32\n","\n","Cost after epoch 32: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 33\n","\n","Cost after epoch 33: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 34\n","\n","Cost after epoch 34: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 35\n","\n","Cost after epoch 35: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 36\n","\n","Cost after epoch 36: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 37\n","\n","Cost after epoch 37: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 38\n","\n","Cost after epoch 38: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 39\n","\n","Cost after epoch 39: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 40\n","\n","Cost after epoch 40: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 41\n","\n","Cost after epoch 41: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 42\n","\n","Cost after epoch 42: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 43\n","\n","Cost after epoch 43: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 44\n","\n","Cost after epoch 44: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 45\n","\n","Cost after epoch 45: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 46\n","\n","Cost after epoch 46: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 47\n","\n","Cost after epoch 47: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 48\n","\n","Cost after epoch 48: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 49\n","\n","Cost after epoch 49: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 50\n","\n","Cost after epoch 50: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 51\n","\n","Cost after epoch 51: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 52\n","\n","Cost after epoch 52: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 53\n","\n","Cost after epoch 53: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 54\n","\n","Cost after epoch 54: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 55\n","\n","Cost after epoch 55: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 56\n","\n","Cost after epoch 56: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 57\n","\n","Cost after epoch 57: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 58\n","\n","Cost after epoch 58: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 59\n","\n","Cost after epoch 59: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 60\n","\n","Cost after epoch 60: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 61\n","\n","Cost after epoch 61: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 62\n","\n","Cost after epoch 62: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 63\n","\n","Cost after epoch 63: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 64\n","\n","Cost after epoch 64: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 65\n","\n","Cost after epoch 65: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 66\n","\n","Cost after epoch 66: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 67\n","\n","Cost after epoch 67: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 68\n","\n","Cost after epoch 68: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 69\n","\n","Cost after epoch 69: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 70\n","\n","Cost after epoch 70: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 71\n","\n","Cost after epoch 71: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 72\n","\n","Cost after epoch 72: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 73\n","\n","Cost after epoch 73: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 74\n","\n","Cost after epoch 74: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 75\n","\n","Cost after epoch 75: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 76\n","\n","Cost after epoch 76: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 77\n","\n","Cost after epoch 77: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 78\n","\n","Cost after epoch 78: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 79\n","\n","Cost after epoch 79: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 80\n","\n","Cost after epoch 80: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 81\n","\n","Cost after epoch 81: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 82\n","\n","Cost after epoch 82: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 83\n","\n","Cost after epoch 83: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 84\n","\n","Cost after epoch 84: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 85\n","\n","Cost after epoch 85: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 86\n","\n","Cost after epoch 86: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 87\n","\n","Cost after epoch 87: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 88\n","\n","Cost after epoch 88: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 89\n","\n","Cost after epoch 89: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 90\n","\n","Cost after epoch 90: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 91\n","\n","Cost after epoch 91: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 92\n","\n","Cost after epoch 92: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 93\n","\n","Cost after epoch 93: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 94\n","\n","Cost after epoch 94: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 95\n","\n","Cost after epoch 95: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 96\n","\n","Cost after epoch 96: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 97\n","\n","Cost after epoch 97: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 98\n","\n","Cost after epoch 98: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 99\n","\n","Cost after epoch 99: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 100\n","\n","Cost after epoch 100: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 101\n","\n","Cost after epoch 101: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 102\n","\n","Cost after epoch 102: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 103\n","\n","Cost after epoch 103: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 104\n","\n","Cost after epoch 104: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 105\n","\n","Cost after epoch 105: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 106\n","\n","Cost after epoch 106: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 107\n","\n","Cost after epoch 107: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 108\n","\n","Cost after epoch 108: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 109\n","\n","Cost after epoch 109: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 110\n","\n","Cost after epoch 110: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 111\n","\n","Cost after epoch 111: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 112\n","\n","Cost after epoch 112: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 113\n","\n","Cost after epoch 113: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 114\n","\n","Cost after epoch 114: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 115\n","\n","Cost after epoch 115: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 116\n","\n","Cost after epoch 116: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 117\n","\n","Cost after epoch 117: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 118\n","\n","Cost after epoch 118: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 119\n","\n","Cost after epoch 119: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 120\n","\n","Cost after epoch 120: nan\n","Computing accuracy on validation set...\n","Accuracy on validation set: 0.004366812227074236\n","Accuracy: 0.004366812227074236\n","F1 Score: 0.00011441647597254005\n","Epoch 121\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-245-26dca616751a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             validation_data=(x_test, y_test))\n\u001b[0m","\u001b[0;32m<ipython-input-210-0ffb4e4607dc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, x_train, y_train, mini_batch_size, learning_rate, num_epochs, validation_data)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0mmini_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmini_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                 \u001b[0mepoch_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_batch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-210-0ffb4e4607dc>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, x_train, y_train, learning_rate, step)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0ma_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward_prop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_cost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_last\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-210-0ffb4e4607dc>\u001b[0m in \u001b[0;36mforward_prop\u001b[0;34m(self, x, training)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-208-8f6a71588e80>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, a_prev, training)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["#Fishers Linear Discriminant Analysis"],"metadata":{"id":"ni_4D42qPww3"}},{"cell_type":"code","source":["import numpy as np\n","\n","class LDA:\n","    def __init__(self, n_components):\n","        self.n_components = n_components\n","        self.linear_discriminants = None\n","        self.predicted=[]\n","    def fit(self, X, y):\n","        n_features = X.shape[1]\n","        class_labels = np.unique(y)\n","\n","        mean_overall = np.mean(X, axis=0)\n","        SW = np.zeros((n_features, n_features))\n","        SB = np.zeros((n_features, n_features))\n","        for c in class_labels:\n","            X_c = X[y == c]\n","            mean_c = np.mean(X_c, axis=0)\n","            # (4, n_c) * (n_c, 4) = (4,4) -> transpose\n","            SW += (X_c - mean_c).T.dot((X_c - mean_c))\n","\n","            # (4, 1) * (1, 4) = (4,4) -> reshape\n","            n_c = X_c.shape[0]\n","            mean_diff = (mean_c - mean_overall).reshape(n_features, 1)\n","            SB += n_c * (mean_diff).dot(mean_diff.T)\n","\n","        A = np.linalg.inv(SW).dot(SB)\n","\n","        eigenvalues, eigenvectors = np.linalg.eig(A)\n","        eigenvectors = eigenvectors.T\n","        idxs = np.argsort(abs(eigenvalues))[::-1]\n","        eigenvalues = eigenvalues[idxs]\n","        eigenvectors = eigenvectors[idxs]\n","        # store first n eigenvectors\n","        self.linear_discriminants = eigenvectors[0 : self.n_components]\n","\n","    def transform(self, X):\n","        # project data\n","        return np.dot(X, self.linear_discriminants.T),self.linear_discriminants\n","    def predict(self, X_projected):\n","        y_pred = [x for x in X]\n","        return np.array(y_pred)\n","    def accuracy(y_true, y_pred):\n","        accuracy = np.sum(y_true == y_pred) / len(y_true)\n","        return accuracy\n","\n","    # Project the data onto the 2 primary linear discriminants\n"],"metadata":{"id":"ua-DdDTIqw1F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X =pneu_train_vdata\n","y = np.ravel(pneu_train_labels)\n","a=len(np.unique(y))\n","lda = LDA(2)\n","lda.fit(X, y)\n","X_projected= lda.transform(pneu_test_vdata)\n","y_pred=lda.predict(X_projected)\n","accu=lda.accuracy((np.ravel(pneu_test_labels)),y_pred)\n","\n","print(\"Shape of X:\", X.shape)\n","print(\"Shape of transformed X:\", X_projected.shape)\n","\n","x1, x2 = X_projected[:, 0], X_projected[:, 1]\n","\n","plt.scatter(\n","    x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 2)\n",")\n","plt.title(\"Pneumonia MNIST\")\n","plt.xlabel(\"Linear Discriminant 1\")\n","plt.ylabel(\"Linear Discriminant 2\")\n","plt.colorbar()\n","plt.show()\n","\n","# multi_class_lda = MultiClassLDA()\n","# multi_class_lda.plot_in_2d(X, y,a,title=\"LDA\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"E1LNsi85T3cp","executionInfo":{"status":"error","timestamp":1651297455291,"user_tz":-330,"elapsed":596,"user":{"displayName":"vignan kumar","userId":"03615945832465291219"}},"outputId":"583e819f-bab9-400f-8b9c-273e1566d899"},"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-614dc64449a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mX_projected\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpneu_test_vdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_projected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0maccu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpneu_test_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Shape of X:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: accuracy() takes 2 positional arguments but 3 were given"]}]},{"cell_type":"code","source":["X = blood_train_vdata\n","y = np.ravel(blood_train_labels)\n","lda = LDA(2)\n","lda.fit(X, y)\n","X_projected = lda.transform(X)\n","\n","print(\"Shape of X:\", X.shape)\n","print(\"Shape of transformed X:\", X_projected.shape)\n","\n","x1, x2 = X_projected[:, 0], X_projected[:, 1]\n","\n","plt.scatter(\n","    x1, x2, c=y, edgecolor=\"none\", alpha=0.8, cmap=plt.cm.get_cmap(\"viridis\", 8)\n",")\n","plt.title(\"Blood MNIST\")\n","plt.xlabel(\"Linear Discriminant 1\")\n","plt.ylabel(\"Linear Discriminant 2\")\n","plt.colorbar()\n","plt.show()"],"metadata":{"id":"mjecwSFwpg6f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"iOYojO4KIcIQ"},"execution_count":null,"outputs":[]}]}